<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring MVC执行流程]]></title>
    <url>%2F2019%2F02%2F28%2FSpring-MVC%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[概述SpringMVC是一个MVC的开源框架，SpringMVC就相当于是Struts2加上Spring的整合，SpringMVC是Spring的一个后续产品，是在Spring在原有基础上，又提供了web应用的MVC模块，可以简单的把SpringMVC理解为是Spring的一个模块（类似AOP，IOC这样的模块）。 流程图 源码调用 原理图 时序图 核心类DispatcherServlet、HandlerMapping、HandlerAdapter、ViewResolver等。 源码包 org.springframework.web.servlet123456789101112131415protected void onRefresh(ApplicationContext context) &#123; this.initStrategies(context); &#125; protected void initStrategies(ApplicationContext context) &#123; this.initMultipartResolver(context); this.initLocaleResolver(context); this.initThemeResolver(context); this.initHandlerMappings(context); this.initHandlerAdapters(context); this.initHandlerExceptionResolvers(context); this.initRequestToViewNameTranslator(context); this.initViewResolvers(context); this.initFlashMapManager(context); &#125; 继承关系： DispatcherServlet extends FrameworkServlet FrameworkServlet extends HttpServletBean HttpServletBean extends HttpServlet 参考文献 spring MVC流程图 SpringMVC 之一：SpringMVC 图解]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试核心问题汇总（持续更新）]]></title>
    <url>%2F2019%2F02%2F25%2F%E9%9D%A2%E8%AF%95%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[基础知识如何理解对象的创建过程 请考虑一个名为Dog 的类： (1) 类型为Dog 的一个对象首次创建时，或者Dog 类的static 方法／static 字段首次访问时，Java 解释器必须找到Dog.class（在事先设好的类路径里搜索）。 (2) 找到Dog.class 后（它会创建一个Class 对象，这将在后面学到），它的所有static 初始化模块都会运行。因此，static 初始化仅发生一次——在Class 对象首次载入的时候。 (3) 创建一个new Dog()时，Dog 对象的构建进程首先会在内存堆（Heap）里为一个Dog 对象分配足够多的存储空间。 (4) 这种存储空间会清为零，将Dog 中的所有基本类型设为它们的默认值（零用于数字，以及boolean 和char 的等价设定）。 (5) 进行字段定义时发生的所有初始化都会执行。 (6) 执行构建器。 servlet中/*和/有什么区别 /*属于路径匹配，并且可以匹配所有request，由于路径匹配的优先级仅次于精确匹配，所以/*会覆盖所有的扩展名匹配，很多404错误均由此引起，所以这是一种特别恶劣的匹配模式，一般只用于filter的url-pattern /是servlet中特殊的匹配模式，切该模式有且仅有一个实例，优先级最低，不会覆盖其他任何url-pattern，只是会替换servlet容器的内建default servlet ，该模式同样会匹配所有request。 配置/后，一种可能的现象是myServlet会拦截诸http://localhost:8080/appDemo/user/addUser.action、http://localhost:8080/appDemo/user/updateUser的格式的请求，但是并不会拦截http://localhost:8080/appDemo/user/users.jsp、http://localhost:8080/appDemo/index.jsp，这是应为servlet容器有内置的*.jsp匹配器，而扩展名匹配的优先级高于缺省匹配，所以才会有上述现象。 如何理解Java关键字this 首先，this关键字必须放在非静态方法里面。 可以使用this关键字引用成员变量。 使用this关键字在自身构造方法内部引用其它构造方法。 使用this关键字代表自身类的对象。 使用this关键字引用成员方法。 this关键字的作用this关键字是一个对象的默认引用。作为对象的默认引用，有两种情形： 在构造器中，引用该构造器执行初始化的对象。 在方法中，引用调用该方法的对象。 构造方法的作用首先每个类都有一个无参数的默认构造函数。这也是为什么有些类即便没有写构造方法也可以通过new来实例化。 为了初始化成员属性，而不是初始化对象，初始化对象是通过new关键字实现的。 通过new调用构造方法初始化对象，编译时根据参数签名来检查构造函数。 创建子类对象会调用父类构造方法但不会创建父类对象，只是调用父类构造方法初始化父类成员属性； 注意事项 继承中的构造方法，有下面这些规则需要遵守 1.子类的构造过程中必须调用其父类的构造方法 2.子类可以在自己的构造方法中使用super(参数列表)调用父类的构造方法 (1).可以使用this(参数列表)调用自己这个类的其他的构造方法调用this的时候也调用了父类的构造方法 (2).如果调用了super，必须写在子类构造方法的第一行 (3).this也必须放在第一行,所以this,super在一个构造方法里无法同时出现实际上严格来讲，还是能实现的，用内部类。在内部类里面，可以使用一个this或者super 3.如果子类的构造方法中没有显示地调用父类构造方法，系统默认调用父类那个无参数的构造方法 4.如果子类构造方法中既没有显示调用父类构造方法，而父类中又没有无参的构造方法，编译会报错。 构造器中一定不要创建自身的实例，否则会造成调用栈溢出错误。 构造器中一定不要创建自身的实例，否则会造成调用栈溢出错误。 123456789class a&#123; a _a = new a(); public a() &#123; _a = new a(); a _b = new a(); &#125;&#125; 以上三种情况都会造成栈溢出，会造成一个无穷递归的调用栈。 基础扩展servlet的url-pattern如何匹配的 servlet容器中的匹配规则既不是简单的通配，也不是正则表达式，而是特定的规则。所以不要用通配符或者正则表达式的匹配规则来看待servlet的url-pattern。 Servlet 2.5开始，一个servlet可以使用多个url-pattern规则，标签声明了与该servlet相应的匹配规则，每个标签代表1个匹配规则； 当servlet容器接收到浏览器发起的一个url请求后，容器会用url减去当前应用的上下文路径，以剩余的字符串作为servlet映射，假如url是http://localhost:8080/appDemo/index.html，其应用上下文是appDemo，容器会将http://localhost:8080/appDemo去掉，用剩下的/index.html部分拿来做servlet的映射匹配。 url-pattern映射匹配过程是有优先顺序的而且当有一个servlet匹配成功以后，就不会去理会剩下的servlet了。 匹配顺序 精确匹配 1234servlet-mapping1：&lt;url-pattern&gt;/user/users.html&lt;/url-pattern&gt;servlet-mapping2：&lt;url-pattern&gt;/*&lt;/url-pattern&gt; 当一个请求http://localhost:8080/appDemo/user/users.html来的时候，servlet-mapping1匹配到，不再用servlet-mapping2匹配。 路径匹配，先最长路径匹配，再最短路径匹配 1234servlet-mapping1：&lt;url-pattern&gt;/user/*&lt;/url-pattern&gt;servlet-mapping2：&lt;url-pattern&gt;/*&lt;/url-pattern&gt; 当一个请求http://localhost:8080/appDemo/user/users.html来的时候，servlet-mapping1匹配到，不再用servlet-mapping2匹配。 扩展名匹配 1234servlet-mapping1：&lt;url-pattern&gt;/user/*&lt;/url-pattern&gt;servlet-mapping2：&lt;url-pattern&gt;*.action&lt;/url-pattern&gt; 当一个请求http://localhost:8080/appDemo/user/addUser.action来的时候，servlet-mapping1匹配到，不再用servlet-mapping2匹配。 缺省匹配以上都找不到servlet，就用默认的servlet，配置为&lt;url-pattern&gt;/&lt;/url-pattern&gt; 如何理解快速失败fail-fast fail-fast机制是java容器（Collection and Map都存在fail-fast机制）中的一种错误机制。在遍历一个容器对象时，当容器结构被修改，很有可能会抛出ConcurrentModificationException的，产生快速失败的。在以下两种情况下会导致快速失败，抛出ConcurrentModificationException 环境单线程 遍历一个集合过程中，集合结构被修改。注意，listIterator.remove（）方法修改集合结构不会抛出这个异常。 线程多环境 当一个线程遍历集合过程中，而另一个线程对集合结构进行了修改。 如何理解CAP理论（AP架构、CP架构） Feature Nacos Eureka Zookeeper 服务健康检查 支持 可配支持 (弱)长连接，keepalive CAP AP AP CP watch支持（客户端观察到服务提供者变化） 支持 支持 long polling/大部分增量 支持 自我保护 支持 支持 - 客户端缓存 支持 支持 - 自身集群的监控 - metrics - CAP理论提出就是针对分布式数据库环境的，所以，P(隔离)这个属性是必须具备的。 P就是在分布式环境中，由于网络的问题可能导致某个节点和其它节点失去联系，这时候就形成了P(partition)，也就是由于网络问题，将系统的成员隔离成了2个区域，互相无法知道对方的状态，这在分布式环境下是非常常见的。因为P是必须的，那么我们需要选择的就是A和C。 大家知道，在分布式环境下，为了保证系统可用性，通常都采取了复制的方式，避免一个节点损坏，导致系统不可用。那么就出现了每个节点上的数据出现了很多个副本的情况，而数据从一个节点复制到另外的节点时需要时间和要求网络畅通的，所以，当P发生时，也就是无法向某个节点复制数据时，这时候你有两个选择：选择可用性 A(Availability)，此时，那个失去联系的节点依然可以向系统提供服务，不过它的数据就不能保证是同步的了（失去了C属性）。 选择一致性C(Consistency)，为了保证数据库的一致性，我们必须等待失去联系的节点恢复过来，在这个过程中，那个节点是不允许对外提供服务的，这时候系统处于不可用状态(失去了A属性)。 最常见的例子是读写分离，某个节点负责写入数据，然后将数据同步到其它节点，其它节点提供读取的服务，当两个节点出现通信问题时，你就面临着选择A（继续提供服务，但是数据不保证准确），C（用户处于等待状态，一直等到数据同步完成）。 Zookeeper为CP设计，Eureka为AP设计，而阿里的Nacos则比Eureka更加强大 。 注： 2018.7，Netflix 公司在 github 上公告 Eureka 2.0 开源工作停止。 如何理解CAS无锁机制 CAS，Compare And Swap，即比较并交换。Doug lea大神在同步组件中大量使用CAS技术鬼斧神工地实现了Java多线程的并发操作。整个AQS同步组件、Atomic原子类操作等等都是以CAS实现的，甚至ConcurrentHashMap在1.8的版本中也调整为了CAS+Synchronized。 Redis为什么Redis是单线程 官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。 缓存穿透、缓存雪崩、缓存预热 缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。 通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 多线程多线程有几种实现方式 继承Thread类 1234567891011121314151617181920212223242526272829public class ThreadDemo extends Thread &#123; @Override public void run() &#123; while (true) &#123; System.out.println(Thread.currentThread().getName() + " is running ... "); // 打印当前线程的名字 try &#123; Thread.sleep(1000); // 休息1000ms &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; ThreadDemo td = new ThreadDemo(); td.start(); // 启动线程 while (true) &#123; System.out.println(Thread.currentThread().getName() + " is running ... "); // 打印当前线程的名字 try &#123; Thread.sleep(1000); // 休息1000ms &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 实现Runnable接口 123456789101112131415public class ThreadTarget implements Runnable &#123; @Override public void run() &#123; while(true) &#123; System.out.println(Thread.currentThread().getName() + " is running .. "); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 内部类的方式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class DemoThread &#123; public static void main(String[] args) &#123; // 基于子类的实现 new Thread() &#123; public void run() &#123; while (true) &#123; System.out.println(Thread.currentThread().getName() + " is running ... "); // 打印当前线程的名字 try &#123; Thread.sleep(1000); // 休息1000ms &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; &#125;.start(); // 基于接口的实现 new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; System.out.println(Thread.currentThread().getName() + " is running ... "); // 打印当前线程的名字 try &#123; Thread.sleep(1000); // 休息1000ms &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); // 主线程的方法 while (true) &#123; System.out.println(Thread.currentThread().getName() + " is running ... "); // 打印当前线程的名字 try &#123; Thread.sleep(1000); // 休息1000ms &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 实现Callable接口现Callable接口,增加了异常和返回值,需要创建一个FutureTask，指定Callable对象，做为线程任务。 12345678910111213141516171819202122public class CallableTest &#123; public static void main(String[] args) throws Exception &#123; Callable&lt;Integer&gt; call = new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; System.out.println("thread start .. "); Thread.sleep(2000); return 1; &#125; &#125;; FutureTask&lt;Integer&gt; task = new FutureTask&lt;&gt;(call); Thread t = new Thread(task); t.start(); System.out.println("do other thing .. "); System.out.println("拿到线程的执行结果 ： " + task.get()); &#125;&#125; Callable中可以通过范型参数来指定线程的返回值类型。通过FutureTask的get方法拿到线程的返回值。 线程池的方式 123456789101112131415161718192021222324public class ThreadPoolDemo &#123; public static void main(String[] args) &#123; // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); while(true) &#123; threadPool.execute(new Runnable() &#123; // 提交多个线程任务，并执行 @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " is running .."); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125;&#125; Spring方式：这种方式依赖于Spring3以上版本，我们可以通过Spring的@Async注解非常方便的实现多线程。 Callable和Runnable的区别 创建执行线程的方式：实现 Callable 接口。 相较于实现 Runnable 接口的方式，方法可以有返回值，并且可以抛出异常。 执行 Callable 方式，需要 FutureTask 实现类的支持，用于接收运算结果。FutureTask是Future接口的实现类，Callable用于产生结果，Future用于获取结果。 CyclicBarrier和CountDownLatch的区别 CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。 如何在两个线程间共享数据 可以通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构，或者根据java内存模型使用volatile生命变量。 什么是FutureTask 在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 synchronized和ReentrantLock的区别 ava在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 Java中Semaphore是什么 Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 如果你提交任务时，线程池队列已满。会时发会生什么 如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。也就是默认的拒绝策略。 什么是阻塞式方法 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 如何在Java中创建Immutable对象 不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 Java中的fork join框架是什么 fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 Executors类是什么 Executors为Executor，ExecutorService，ScheduledExecutorService，ThreadFactory和Callable类提供了一些工具方法。可以用于方便的创建线程池。 Executor有什么作用 Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架，目的是提供一种将”任务提交”与”任务如何运行”分离开来的机制。 sleep() 、wait（）的区别 sleep（）属于Thread，wait（）属于 Class。 sleep（）仅仅是睡眠，不涉及到锁的释放问题，睡眠时间结束自动恢复到运行状态。 wait（）绑定了某个对象的锁，等待该对象的notify（），notifyAll（）来唤醒自己，等待的时间是未知的，甚至出现死锁。共同点： 都是要释放cpu的，并且在线程池中会占用位置。 分布式锁有几种实现方式 数据库乐观锁； 基于Redis的分布式锁； 基于ZooKeeper的分布式锁。 实现原理synchronized的实现原理 Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。12345678910monitorenter： 每个对象有一个监视器锁（monitor）。当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下：* 1、如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者。* 2、如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1.* 3.如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取 monitor的所有权。monitorexit： 执行monitorexit的线程必须是objectref所对应的monitor的所有者。指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor，不再是这个monitor的所有者。其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权。 volatile的实现原理 在JVM底层volatile是采用“内存屏障”来实现的。12345 “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： * 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； * 它会强制将对缓存的修改操作立即写入主存； * 如果是写操作，它会导致其他CPU中对应的缓存行无效。 AtomicInteger的实现原理 CAS无锁机制，源码： 123456789101112131415161718192021222324 private volatile int value;/* * AtomicInteger内部声明了一个volatile修饰的变量value用来保存实际值 * 使用带参的构造函数会将入参赋值给value，无参构造器value默认值为0 */public AtomicInteger(int initialValue) &#123; value = initialValue;&#125;import sun.misc.Unsafe;private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125;/* * 可以看到自增函数中调用了Unsafe函数的getAndAddInt方法 */public final int incrementAndGet() &#123; return unsafe.getAndAddInt(this, valueOffset, 1) + 1;&#125; 说明 Unsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。Unsafe类使Java拥有了像C语言的指针一样操作内存空间的能力，同时也带来了指针的问题。过度的使用Unsafe类会使得出错的几率变大，因此Java官方并不建议使用的，官方文档也几乎没有。通常我们最好也不要使用Unsafe类，除非有明确的目的，并且也要对它有深入的了解才行。CAS也并非完美的，它会导致ABA问题，就是说，当前内存的值一开始是A，被另外一个线程先改为B然后再改为A，那么当前线程访问的时候发现是A，则认为它没有被其他线程访问过。在某些场景下这样是存在错误风险的。比如在链表中。那么如何解决这个ABA问题呢，大多数情况下乐观锁的实现都会通过引入一个版本号标记这个对象，每次修改版本号都会变话，比如使用时间戳作为版本号，这样就可以很好的解决ABA问题。 HashMap的实现原理 HashMap底层使用数组实现，数组中每一项是个单向链表，即数组和链表的结合体；当链表长度大于一定阈值时，链表转换为红黑树，这样减少链表查询时间。 HashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Node对象。HashMap底层采用一个Node[]数组来保存所有的key-value对，当需要存储一个Node对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Node时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Node。 ConcurrentHashMap的实现原理 JDK1.7： ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的HashTable,它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。把一个整体分成了16个段(Segment.也就是最高支持16个线程的并发修改操作。这也是在重线程场景时减小锁的粒度从而降低锁竞争的一种方案。并且代码中大多共享变量使用volatile关键字声明，目的是第一时间获取修改的内容，性能非常好。 JDK1.8： 1.8中放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现。 Method.invoke()实现原理 Method.invoke()实际上并不是自己实现的反射调用逻辑，而是委托给sun.reflect.MethodAccessor来处理。首先要了解Method对象的基本构成，每个Java方法有且只有一个Method对象作为root，它相当于根对象，对用户不可见。当我们创建Method对象时，我们代码中获得的Method对象都相当于它的副本（或引用）。root对象持有一个MethodAccessor对象，所以所有获取到的Method对象都共享这一个MethodAccessor对象，因此必须保证它在内存中的可见性。 索引的实现原理BTree结构 一般来说B+Tree比BTree更适合实现外存的索引结构，因为存储引擎的设计专家巧妙的利用了外存（磁盘）的存储结构，即磁盘的一个扇区是整数倍的page（页），页是存储中的一个单位，通常默认为4K，因此索引结构的节点被设计为一个页的大小，然后利用外存的“预读取”原则，每次读取的时候，把整个节点的数据读取到内存中，然后在内存中查找，已知内存的读取速度是外存读取I/O速度的几百倍，那么提升查找速度的关键就在于尽可能少的磁盘I/O，那么可以知道，每个节点中的key个数越多，那么树的高度越小，需要I/O的次数越少，因此一般来说B+Tree比BTree更快，因为B+Tree的非叶节点中不存储data，就可以存储更多的key。 MyBatis的实现原理 MyBatis应用程序通过SqlSessionFactoryBuilder从mybatis-config.xml配置文件（也可以用Java文件配置的方式，需要添加@Configuration）中构建出SqlSessionFactory（SqlSessionFactory是线程安全的）。然后，SqlSessionFactory的实例直接开启一个SqlSession，再通过SqlSession实例获得Mapper对象并运行Mapper映射的SQL语句，完成对数据库的CRUD和事务提交，之后关闭SqlSession。说明：SqlSession是单线程对象，因为它是非线程安全的，是持久化操作的独享对象，类似jdbc中的Connection，底层就封装了jdbc连接。 MyBatis从功能架构上可以分为三层： API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 ArrayLis实现原理 首页，ArrayList底层使用数组实现,集合是可变长度数组，初始容量是10，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的1.5倍，这种操作的代价很高。List接口的可变数组非同步实现，并允许包括null在内的所有元素，并且使用到了System.arraycopy方法。 12345678public void add(int index, E element) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException( "Index: "+index+", Size: "+size); ensureCapacity(size+1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 字符串的工作原理 当代码中出现字面量形式创建字符串对象时吗，JVM就会首先对这个字面量进行检查，，如果字符串常量池中存在相同内容的字符串对象的引用，则将这个引用返回，否则新的字符串对象就被创建，然后将这个引用放入字符串常量池中，并返回该引用。 JVM的工作原理 装载 装载过程负责找到二进制字节码并加载至JVM中，JVM通过类名、类所在的包名通过ClassLoader来完成类的加载，同样，也采用以上三个元素来标识一个被加载了的类：类名+包名+ClassLoader实例ID。 链接 链接过程负责对二进制字节码的格式进行校验、初始化装载类中的静态变量以及解析类中调用的接口、类。在完成了校验后，JVM初始化类中的静态变量，并将其值赋为默认值。最后一步为对类中的所有属性、方法进行验证，以确保其需要调用的属性、方法存在，以及具备应的权限（例如public、private域权限等），会造成NoSuchMethodError、NoSuchFieldError等错误信息。 初始化 初始化过程即为执行类中的静态初始化代码、构造器代码以及静态属性的初始化，在四种情况下初始化过程会被触发执行：调用了new；反射调用了类中的方法；子类调用了初始化；JVM启动过程中指定的初始化类。 Redis工作原理 Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库,可以达到100000+的QPS（每秒内查询次数)。之所以QPS如此高，有以下几点： 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 相关： 单进程多线程模型：MySQL、Memcached、Oracle（Windows版本）； 多进程模型：Oracle（Linux版本）； Nginx有两类进程，一类称为Master进程(相当于管理进程)，另一类称为Worker进程（实际工作进程）。 Spring Boot Starter的工作原理 参考文献 Mybatis实现原理深入解析 JVM工作原理 java多线程的6种实现方式详解]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
        <tag>多线程</tag>
        <tag>工作原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA线程池]]></title>
    <url>%2F2019%2F02%2F24%2FJAVA%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程池概述java.util.concurrent.Executors提供了一个 java.util.concurrent.Executor接口的实现用于创建线程池多线程技术主要解决处理器单元内多个线程执行的问题，它可以显著减少处理器单元的闲置时间，增加处理器单元的吞吐能力。 实现原理java线程池的实现原理很简单，说白了就是一个线程集合workerSet和一个阻塞队列workQueue。当用户向线程池提交一个任务(也就是线程)时，线程池会先将任务放入workQueue中。workerSet中的线程会不断的从workQueue中获取线程然后执行。当workQueue中没有任务的时候，worker就会阻塞，直到队列中有任务了就取出来继续执行。 线程池的作用线程池作用就是限制系统中执行线程的数量。根据系统的环境情况，可以自动或手动设置线程数量，达到运行的最佳效果；少了浪费了系统资源，多了造成系统拥挤效率不高。用线程池控制线程数量，其他线程排队等候。一个任务执行完毕，再从队列的中取最前面的任务开始执行。若队列中没有等待进程，线程池的这一资源处于等待。当一个新任务需要运行时，如果线程池中有等待的工作线程，就可以开始运行了；否则进入等待队列。 线程池的优点 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 便于统一管理、分配、调优和监控。 Java1.5中引入的Executor框架把任务的提交和执行进行解耦，只需要定义好任务，然后提交给线程池，而不用关心该任务是如何执行、被哪个线程执行，以及什么时候执行。 Executor框架接口Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架，目的是提供一种将”任务提交”与”任务如何运行”分离开来的机制。 J.U.C中有三个Executor接口： Executor：一个运行新任务的简单接口； ExecutorService：扩展了Executor接口。添加了一些用来管理执行器生命周期和任务生命周期的方法； ScheduledExecutorService：扩展了ExecutorService。支持Future和定期执行任务。 主要参数1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize: 核心线程数量，即规定线程池有几个线程(worker)在运行。 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务；所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize。 maximumPoolSize: 最大线程数量。当workQueue满了,不能添加任务的时候，这个参数才会生效。规定线程池最多只能有多少个线程(worker)在执行。 keepAliveTime: 线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime。 unit: 生存时间对于的单位 workQueue: 等待队列，当任务提交时，如果线程池中的线程数量大于等于corePoolSize的时候，把该任务封装成一个Worker对象放入等待队列。主要有以下几种处理方式： 直接切换：这种方式常用的队列是SynchronousQueue，但现在还没有研究过该队列，这里暂时还没法介绍； 使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了（后面也会说到）。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。 threadFactory: 它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。 handler: 它是RejectedExecutionHandler类型的变量，表示线程池的拒绝策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 用户通过submit提交一个任务。线程池会执行如下流程: 判断当前运行的worker数量是否超过corePoolSize,如果不超过corePoolSize。就创建一个worker直接执行该任务。—— 线程池最开始是没有worker在运行的 。 如果正在运行的worker数量超过或者等于corePoolSize,那么就将该任务加入到workQueue队列中去。 如果workQueue队列满了,也就是offer方法返回false的话，就检查当前运行的worker数量是否小于maximumPoolSize,如果小于就创建一个worker直接执行该任务。 如果当前运行的worker数量是否大于等于maximumPoolSize，那么就执行RejectedExecutionHandler来拒绝这个任务的提交。 线程池常见的5种状态 RUNNING ：能接受新提交的任务，并且也能处理阻塞队列中的任务； SHUTDOWN：关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务。在线程池处于RUNNING 状态时，调用 shutdown()方法会使线程池进入到该状态。（finalize() 方法在执行过程中也会调用 shutdown()方法进入该状态）； STOP：不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态； TIDYING：如果所有的任务都已终止了，workerCount (有效线程数) 为0，线程池进入该状态后会调用terminated() 方法进入TERMINATED 状态。 TERMINATED：在terminated() 方法执行完后进入该状态，默认terminated()方法中什么也没有做。 进入TERMINATED的条件如下： 线程池不是RUNNING状态； 线程池状态不是TIDYING状态或TERMINATED状态； 如果线程池状态是SHUTDOWN并且workerQueue为空； workerCount为0； 设置TIDYING状态成功。 源码解析ThreadPoolExecutor ThreadPoolExecutor中的关键属性1234567891011121314151617//这个属性是用来存放 当前运行的worker数量以及线程池状态的//int是32位的，这里把int的高3位拿来充当线程池状态的标志位,后29位拿来充当当前运行worker的数量private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//存放任务的阻塞队列private final BlockingQueue&lt;Runnable&gt; workQueue;//worker的集合,用set来存放private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();//历史达到的worker数最大值private int largestPoolSize;//当队列满了并且worker的数量达到maxSize的时候,执行具体的拒绝策略private volatile RejectedExecutionHandler handler;//超出coreSize的worker的生存时间private volatile long keepAliveTime;//常驻worker的数量private volatile int corePoolSize;//最大worker的数量,一般当workQueue满了才会用到这个参数private volatile int maximumPoolSize; 提交任务 提交任务时处理过程相关源码12345678910111213141516171819202122232425262728public void execute(Runnable command) &#123; if (command == ) throw new NullPointerException(); int c = ctl.get(); //workerCountOf(c)会获取当前正在运行的worker数量 if (workerCountOf(c) &lt; corePoolSize) &#123; //如果workerCount小于corePoolSize,就创建一个worker然后直接执行该任务 if (addWorker(command, true)) return; c = ctl.get(); &#125; //isRunning(c)是判断线程池是否在运行中,如果线程池被关闭了就不会再接受任务 //后面将任务加入到队列中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //如果添加到队列成功了,会再检查一次线程池的状态 int recheck = ctl.get(); //如果线程池关闭了,就将刚才添加的任务从队列中移除 if (! isRunning(recheck) &amp;&amp; remove(command)) //执行拒绝策略 reject(command); else if (workerCountOf(recheck) == 0) addWorker(, false); &#125; //如果加入队列失败,就尝试直接创建worker来执行任务 else if (!addWorker(command, false)) //如果创建worker失败,就执行拒绝策略 reject(command);&#125; addWorker 添加worker的方法addWorker源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: //使用自旋+cas失败重试来保证线程竞争问题 for (;;) &#123; //先获取线程池的状态 int c = ctl.get(); int rs = runStateOf(c); // 如果线程池是关闭的,或者workQueue队列非空,就直接返回false,不做任何处理 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //根据入参core 来判断可以创建的worker数量是否达到上限,如果达到上限了就拒绝创建worker if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //没有的话就尝试修改ctl添加workerCount的值。这里用了cas操作,如果失败了下一个循环会继续重试,直到设置成功 if (compareAndIncrementWorkerCount(c)) //如果设置成功了就跳出外层的那个for循环 break retry; //重读一次ctl,判断如果线程池的状态改变了,会再重新循环一次 c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = ; try &#123; final ReentrantLock mainLock = this.mainLock; //创建一个worker,将提交上来的任务直接交给worker w = new Worker(firstTask); final Thread t = w.thread; if (t != ) &#123; //加锁,防止竞争 mainLock.lock(); try &#123; int c = ctl.get(); int rs = runStateOf(c); //还是判断线程池的状态 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == )) &#123; //如果worker的线程已经启动了,会抛出异常 if (t.isAlive()) throw new IllegalThreadStateException(); //添加新建的worker到线程池中 workers.add(w); int s = workers.size(); //更新历史worker数量的最大值 if (s &gt; largestPoolSize) largestPoolSize = s; //设置新增标志位 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; //如果worker是新增的,就启动该线程 if (workerAdded) &#123; t.start(); //成功启动了线程,设置对应的标志位 workerStarted = true; &#125; &#125; &#125; finally &#123; //如果启动失败了,会触发执行相应的方法 if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; Worker Worker是ThreadPoolExecutor内部定义的一个内部类。Worker的继承关系如下：1private final class Worker extends AbstractQueuedSynchronizer implements Runnable 它实现了Runnable接口,所以可以拿来当线程用。同时它还继承了AbstractQueuedSynchronizer同步器类,主要用来实现一个不可重入的锁。 构造方法 123456789101112//运行的线程,前面addWorker方法中就是直接通过启动这个线程来启动这个workerfinal Thread thread;//当一个worker刚创建的时候,就先尝试执行这个任务Runnable firstTask;//记录完成任务的数量volatile long completedTasks;Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; //创建一个Thread,将自己设置给他,后面这个thread启动的时候,也就是执行worker的run方法 this.thread = getThreadFactory().newThread(this);&#125; run方法 1234public void run() &#123; //这里调用了ThreadPoolExecutor的runWorker方法 runWorker(this);&#125; ThreadPoolExecutor的runWorker方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final void runWorker(Worker w) &#123; //获取当前线程 Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = ; //执行unlock方法,允许其他线程来中断自己 w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; //如果前面的firstTask有值,就直接执行这个任务 //如果没有具体的任务,就执行getTask()方法从队列中获取任务 //这里会不断执行循环体,除非线程中断或者getTask()返回null才会跳出这个循环 while (task != || (task = getTask()) != ) &#123; //执行任务前先锁住,这里主要的作用就是给shutdown方法判断worker是否在执行中的 //shutdown方法里面会尝试给这个线程加锁,如果这个线程在执行,就不会中断它 w.lock(); //判断线程池状态,如果线程池被强制关闭了,就马上退出 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; //执行任务前调用。预留的方法,可扩展 beforeExecute(wt, task); Throwable thrown = ; try &#123; //真正的执行任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; //执行任务后调用。预留的方法,可扩展 afterExecute(task, thrown); &#125; &#125; finally &#123; task = ; //记录完成的任务数量 w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; getTask()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private Runnable getTask() &#123; boolean timedOut = false; for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 如果线程池已经关闭了,就直接返回null, //如果这里返回null,调用的那个worker就会跳出while循环,然后执行完销毁线程 //SHUTDOWN状态表示执行了shutdown()方法 //STOP表示执行了shutdownNow()方法 if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return ; &#125; //获取当前正在运行中的worker数量 int wc = workerCountOf(c); // 如果设置了核心worker也会超时或者当前正在运行的worker数量超过了corePoolSize,就要根据时间判断是否要销毁线程了 //其实就是从队列获取任务的时候要不要设置超时间时间,如果超过这个时间队列还没有任务进来,就会返回null boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; //如果上一次循环从队列获取到的未null,这时候timedOut就会为true了 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; //通过cas来设置WorkerCount,如果多个线程竞争,只有一个可以设置成功 //最后如果没设置成功,就进入下一次循环,说不定下一次worker的数量就没有超过corePoolSize了,也就不用销毁worker了 if (compareAndDecrementWorkerCount(c)) return ; continue; &#125; try &#123; //如果要设置超时时间,就设置一下咯 //过了这个keepAliveTime时间还没有任务进队列就会返回null,那worker就会销毁 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != ) return r; //如果r为null,就设置timedOut为true timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 添加Callable任务的实现源码 123456public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == ) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 要添加一个有返回值的任务的实现也很简单。其实就是对任务做了一层封装，将其封装成Future，然后提交给线程池执行，最后返回这个Future。这里的 newTaskFor(task) 方法会将其封装成一个FutureTask类。外部的线程拿到这个Future，执行get()方法的时候,如果任务本身没有执行完，执行线程就会被阻塞，直到任务执行完。 FutureTask的get方法12345678public V get() throws InterruptedException, ExecutionException &#123; int s = state; //判断状态,如果任务还没执行完,就进入休眠,等待唤醒 if (s &lt;= COMPLETING) s = awaitDone(false, 0L); //返回值 return report(s);&#125; FutureTask中通过一个state状态来判断任务是否完成。当run方法执行完后,会将state状态置为完成，同时唤醒所有正在等待的线程。 FutureTask的run方法 12345678910111213141516171819202122232425262728293031public void run() &#123; //判断线程的状态 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, , Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //执行call方法 result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = ; ran = false; setException(ex); &#125; if (ran) //这个方法里面会设置返回内容,并且唤醒所以等待中的线程 set(result); &#125; &#125; finally &#123; runner = ; int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; shutdown方法shutdown方法会将线程池的状态设置为SHUTDOWN,线程池进入这个状态后,就拒绝再接受任务,然后会将剩余的任务全部执行完。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //检查是否可以关闭线程 checkShutdownAccess(); //设置线程池状态 advanceRunState(SHUTDOWN); //尝试中断worker interruptIdleWorkers(); //预留方法,留给子类实现 onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate();&#125;private void interruptIdleWorkers() &#123; interruptIdleWorkers(false);&#125;private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //遍历所有的worker for (Worker w : workers) &#123; Thread t = w.thread; //先尝试调用w.tryLock(),如果获取到锁,就说明worker是空闲的,就可以直接中断它 //注意的是,worker自己本身实现了AQS同步框架,然后实现的类似锁的功能 //它实现的锁是不可重入的,所以如果worker在执行任务的时候,会先进行加锁,这里tryLock()就会返回false if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125; shutdownNow方法 shutdownNow做的比较绝，它先将线程池状态设置为STOP，然后拒绝所有提交的任务。最后中断左右正在运行中的worker,然后清空任务队列。 123456789101112131415161718192021222324252627282930public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); //检测权限 advanceRunState(STOP); //中断所有的worker interruptWorkers(); //清空任务队列 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125;private void interruptWorkers() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //遍历所有worker，然后调用中断方法 for (Worker w : workers) w.interruptIfStarted(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 常见的4种线程池newCachedThreadPool newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; public void run() &#123; System.out.println(index); &#125; &#125;); &#125; &#125; &#125; newFixedThreadPool newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; newScheduledThreadPool newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。1234567891011121314package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(new Runnable() &#123; public void run() &#123; System.out.println("delay 3 seconds"); &#125; &#125;, 1, 3, TimeUnit.SECONDS); //表示延迟1秒后每3秒执行一次。 &#125; &#125; newSingleThreadExecutor newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; 参考文献 Java线程池实现原理详解 深入理解Java线程池：ThreadPoolExecutor]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>线程池</tag>
        <tag>Executor</tag>
        <tag>拒绝策略</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM垃圾回收]]></title>
    <url>%2F2019%2F02%2F23%2FJVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[垃圾回收GC (Garbage Collection)的基本原理：将内存中不再被使用的对象进行回收，GC中用于回收的方法称为收集器，由于GC需要消耗一些资源和时间，Java在对对象的生命周期特征进行分析后，按照新生代、旧生代的方式来对对象进行收集，以尽可能的缩短GC对应用造成的暂停。 JAVA中，GC通过确定对象是否被活动对象引用来确定是否收集该对象。目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别那些对象应放在新生代，那些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 对象引用不同的对象引用类型， GC会采用不同的方法进行回收，JVM对象的引用分为了四种类型： （1）强引用：默认情况下，对象采用的均为强引用，如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空 间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 （2）软引用：如果一个对象只具有软引用，那就类似于可有可物的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。 （3）弱引用：弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 （4）虚引用：“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是 否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 对象存亡判断引用计数法给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们。 可达性分析算法这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 回收器作用垃圾回收器主要负责： 分配内存 保证所有正在被引用的对象还存在于内存中 回收执行代码已经不再引用的对象所占的内存 减少GC开销的措施 1)不要显式调用System.gc()。此函数建议JVM进行主GC,虽然只是建议而非一定,但很多情况下它会触发主GC,从而增加主GC的频率,也即增加了间歇性停顿的次数。大大的影响系统性能。 2)尽量减少临时对象的使用。临时对象在跳出函数调用后,会成为垃圾,少用临时变量就相当于减少了垃圾的产生,从而延长了出现上述第二个触发条件出现的时间,减少了主GC的机会。 3)对象不用时最好显式置为Null。一般而言,为Null的对象都会被作为垃圾处理,所以将不用的对象显式地设为Null,有利于GC收集器判定垃圾,从而提高了GC的效率。 4)尽量使用StringBuffer,而不用String来累加字符串。由于String是固定长的字符串对象,累加String对象时,并非在一个String对象中扩增,而是重新创建新的String对象,如Str5=Str1+Str2+Str3+Str4,这条语句执行过程中会产生多个垃圾对象,因为对次作“+”操作时都必须创建新的String对象,但这些过渡对象对系统来说是没有实际意义的,只会增加更多的垃圾。避免这种情况可以改用StringBuffer来累加字符串,因StringBuffer是可变长的,它在原有基础上进行扩增,不会产生中间对象。 5)能用基本类型如Int,Long,就不用Integer,Long对象。基本类型变量占用的内存资源比相应对象占用的少得多,如果没有必要,最好使用基本变量。 6)尽量少用静态对象变量。静态变量属于全局变量,不会被GC回收,它们会一直占用内存。 7)分散对象创建或删除的时间。集中在短时间内大量创建新对象,特别是大对象,会导致突然需要大量内存,JVM在面临这种情况时,只能进行主GC,以回收内存或整合内存碎片,从而增加主GC的频率。集中删除对象,道理也是一样的。它使得突然出现了大量的垃圾对象,空闲空间必然减少,从而大大增加了下一次创建新对象时强制主GC的机会。 垃圾收集算法 标记-清除算法算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，效率也很高，但是会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 复制算法为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 标记-整理算法根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一段移动，然后直接清理掉端边界以外的内存。 分代收集算法当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清楚”或“标记-整理”算法进行垃圾收集。 常见问题 如何判断一个常量是废弃常量假如在常量池中存在字符串 “abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 如何判断一个类是无用的类判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 设计选择在设计或选择垃圾收集算法时, 必须做出许多选择，一下是官方的三种回收设计模式： 串行与并行对于串行收集, 一次只发生一件事。例如, 即使多个 cpu可用, 只有一个用于执行集合。当使用并行集合时,垃圾回收被拆分为多个部分, 这些子部分在不同的Cpu。同时操作使集合能够更快地完成, 而牺牲了一些额外的复杂性和潜在的碎片。 官方原文Serial versus ParallelWith serial collection, only one thing happens at a time. For example, even when multiple CPUs areavailable, only one is utilized to perform the collection. When parallel collection is used, the task of garbage collection is split into parts and those subparts are executed simultaneously , on different CPUs. The simultaneous operation enables the collection to be done more quickly, at the expense of some additional complexity and potential fragmentation. Memory Management in the JavaHotSpot™ Virtual Machine Sun Microsystems 并行与停止世界执行停止到世界垃圾回收时, 应用程序的执行完全在收集过程中挂起。或者, 可以执行一个或多个垃圾收集任务同时, 即与应用程序同时进行。通常情况下, 并发垃圾回收器同时做它的大部分工作, 但也可能偶尔要做一些短暂的停止世界暂停。停止世界垃圾收集比并发收集更简单, 因为堆是冻结和对象在收集过程中不会更改。它的缺点是, 它可能是某些应用程序不需要暂停。相应地, 暂停时间更短, 当垃圾回收是同时完成的, 但收集器必须格外小心, 因为它正在进行应用程序可能同时更新的对象。这增加了一些开销会影响性能并需要更大的堆大小的并发收集器。 官方原文Concurrent versus Stop-the-worldWhen stop-the-world garbage collection is performed, execution of the application is completelysuspended during the collection. Alternatively, one or more garbage collection tasks can be executedconcurrently, that is, simultaneously, with the application. Typically, a concurrent garbage collector does most of its work concurrently, but may also occasionally have to do a few short stop-the-world pauses. Stop-the-world garbage collection is simpler than concurrent collection, since the heap is frozen and objects are not changing during the collection. Its disadvantage is that it may be undesirable for some applications to be paused. Correspondingly, the pause times are shorter whengarbage collection is done concurrently, but the collector must take extra care, as it is operating over objects that might be updated at the same time by the application. This adds some overhead toconcurrent collectors that affects performance and requires a larger heap size. Memory Management in the JavaHotSpot™ Virtual Machine Sun Microsystems 压实与非压实与复制垃圾回收器确定内存中的哪些对象是活动对象, 哪些是垃圾后, 它可以压缩内存, 将所有的活动对象一起移动在一起, 并完全回收剩余内存。压缩后, 在第一个免费对象上分配一个新对象是很容易和快速的位置。可以使用一个简单的指针来跟踪对象可用的下一个位置分配。与压缩收集器不同的是, 非压实收集器释放空间垃圾对象就地使用, 即它不会移动所有活动对象来创建一个大的回收区域, 就像压实收集器所做的那样。好处是更快地完成垃圾集合, 但缺点是潜在的碎片。一般来说, 分配成本更高从具有就地取消分配的堆, 而不是从压缩的堆。可能有必要搜索堆的连续区域的内存足够大, 以容纳新对象。第三个另一种方法是复制收集器, 将活动对象复制 (或撤离) 到不同的内存区域。这样做的好处是, 源区域可以被认为是空的, 可以快速、方便地使用后续分配, 但缺点是复制所需的额外时间和额外的可能需要的空间。 官方原文Compacting versus Non-compacting versus CopyingAfter a garbage collector has determined which objects in memory are live and which are garbage, itcan compact the memory, moving all the live objects together and completely reclaiming theremaining memory. After compaction, it is easy and fast to allocate a new object at the first freelocation. A simple pointer can be utilized to keep track of the next location available for objectallocation. In contrast with a compacting collector, a non-compacting collector releases the spaceutilized by garbage objects in-place, i.e., it does not move all live objects to create a large reclaimedregion in the same way a compacting collector does. The benefit is faster completion of garbagecollection, but the drawback is potential fragmentation. In general, it is more expensive to allocatefrom a heap with in-place deallocation than from a compacted heap. It may be necessary to search theheap for a contiguous area of memory sufficiently large to accommodate the new object. A thirdalternative is a copying collector, which copies (or evacuates) live objects to a different memory area.The benefit is that the source area can then be considered empty and available for fast and easysubsequent allocations, but the drawback is the additional time required for copying and the extraspace that may be required. 参考文献 Memory Management in the JavaHotSpot™ Virtual Machine 搞定JVM垃圾回收就是这么简单]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
        <tag>引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA 8新特性]]></title>
    <url>%2F2019%2F02%2F21%2FJDK8%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Java 8(又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。 Oracle 公司于 2014 年 3 月 18 日发布 Java 8 ，它支持函数式编程，新的 JavaScript 引擎，新的日期 API，新的Stream API 等。 新特性 Lambda 表达式 − Lambda允许把函数作为一个方法的参数（函数作为参数传递进方法中。 方法引用 − 方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。与lambda联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 新工具 − 新的编译工具，如：Nashorn引擎 jjs、 类依赖分析器jdeps。 Stream API −新添加的Stream API（java.util.stream） 把真正的函数式编程风格引入到Java中。 Date Time API − 加强对日期与时间的处理。 Optional 类 − Optional 类已经成为 Java 8 类库的一部分，用来解决空指针异常。 Nashorn, JavaScript 引擎 − Java 8提供了一个新的Nashorn javascript引擎，它允许我们在JVM上运行特定的javascript应用。 Lambda 表达式概述Lambda 表达式，也可称为闭包，它是推动 Java 8 发布的最重要新特性。Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）。使用 Lambda 表达式可以使代码变的更加简洁紧凑。 语法1(parameters) -&gt; expression 或 (parameters) -&gt;&#123; statements; &#125; 特性 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字：如果主体只有一个表达式返回值则编译器会自动返回值，大括号需要指定明表达式返回了一个数值。 变量作用域lambda 表达式只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。 实例 不需要参数,返回值为 5 () -&gt; 5 接收一个参数(数字类型),返回其2倍的值 x -&gt; 2 * x 接受2个参数(数字),并返回他们的差值 (x, y) -&gt; x – y 接收2个int型整数,返回他们的和 (int x, int y) -&gt; x + y 接受一个 string 对象,并在控制台打印,不返回任何值(看起来像是返回void) (String s) -&gt; System.out.print(s) java 7 排序 12345678private void sortUsingJava7(List&lt;String&gt; names)&#123; Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String s1, String s2) &#123; return s1.compareTo(s2); &#125; &#125;);&#125; java 8 排序 123private void sortUsingJava8(List&lt;String&gt; names)&#123; Collections.sort(names, (s1, s2) -&gt; s1.compareTo(s2));&#125; 编译问题 lambda 表达式的局部变量可以不用声明为 final，但是必须不可被后面的代码修改（即隐性的具有 final 的语义） 123456int num = 1; Converter&lt;Integer, String&gt; s = (param) -&gt; System.out.println(String.valueOf(param + num));s.convert(2);num = 5; //报错信息：Local variable num defined in an enclosing scope must be final or effectively final 在 Lambda 表达式当中不允许声明一个与局部变量同名的参数或者局部变量，编译会出错。 12String first = ""; Comparator&lt;String&gt; comparator = (first, second) -&gt; Integer.compare(first.length(), second.length()); 方法引用方法引用通过方法的名字来指向一个方法。方法引用可以使语言的构造更紧凑简洁，减少冗余代码。方法引用使用一对冒号 :: 。 构造器引用：它的语法是Class::new，或者更一般的Class&lt; T &gt;::new,实例如下： 12final Car car = Car.create( Car::new );final List&lt; Car &gt; cars = Arrays.asList( car ); 静态方法引用：它的语法是Class::static_method，实例如下： 1cars.forEach( Car::collide ) 特定类的任意对象的方法引用：它的语法是Class::method,实例如下： 1cars.forEach( Car::repair ); 特定对象的方法引用：它的语法是instance::method,实例如下： 12final Car police = Car.create( Car::new );cars.forEach( police::follow ); 实例 代码块 12345678910111213141516import java.util.List;import java.util.ArrayList; public class Java8Tester &#123; public static void main(String args[])&#123; List names = new ArrayList(); names.add("Google"); names.add("Runoob"); names.add("Taobao"); names.add("Baidu"); names.add("Sina"); names.forEach(System.out::println); &#125;&#125; 输出结果 12345GoogleRunoobTaobaoBaiduSina 函数式接口函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。函数式接口可以被隐式转换为 lambda 表达式。Lambda 表达式和方法引用（实际上也可认为是Lambda表达式）上。 如定义了一个函数式接口如下： 12345@FunctionalInterfaceinterface GreetingService &#123; void sayMessage(String message);&#125; 那么就可以使用Lambda表达式来表示该接口的一个实现 1GreetingService greetService1 = message -&gt; System.out.println("Hello " + message); (注：JAVA 8 之前一般是用匿名类实现的) JDK 1.8 之前已有的函数式接口: java.lang.Runnable java.util.concurrent.Callable java.security.PrivilegedAction java.util.Comparator java.io.FileFilter java.nio.file.PathMatcher java.lang.reflect.InvocationHandler java.beans.PropertyChangeListener java.awt.event.ActionListener javax.swing.event.ChangeListener JDK 1.8 新增加的函数接口： java.util.function 提醒：加不加 @FunctionalInterface 对于接口是不是函数式接口没有影响，该注解只是提醒编译器去检查该接口是否仅包含一个抽象方法 默认方法Java 8 新增了接口的默认方法。充分展示了Java平台中概念的一致性与JDK向前兼容之间的矛盾。简单说，我们用default来标注默认类，被default所标注的方法，需要提供实现，而子类可以选择实现或者不实现该方法。通过这样的机制，就能够实现在接口中加入新方法，则子类无需进行任何改动，需要注意的是default只能用于接口中修饰方法，不能在类中使用 为什么要有这个特性？ 首先，之前的接口是个双刃剑，好处是面向抽象而不是面向具体编程，缺陷是，当需要修改接口时候，需要修改全部实现该接口的类，目前的 java 8 之前的集合框架没有 foreach 方法，通常能想到的解决办法是在JDK里给相关的接口添加新的方法及实现。然而，对于已经发布的版本，是没法在给接口添加新方法的同时不影响已有的实现。所以引进的默认方法。他们的目的是为了解决接口的修改与现有的实现不兼容的问题。 默认方法的冲突：父类优先原则 接口中的默认方法和继承的父类方法冲突了，那么这个时候会选择父类中的方法，而不是接口中的默认方法。这个也叫做类优先原则，它可以保证与Java7的兼容性。也就是说，在接口中实现的一个默认方法，它不会对Java8之前写的代码产生影响。所以，我们也不能在接口中定义toString()和equals()这样的接口，因为根据类优先的原则，Object中的这些方法会保留。如果一个类实现了多个接口，那么通过使用Override重写的方式解决冲突问题。 StreamJava 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。 123+--------------------+ +------+ +------+ +---+ +-------+| stream of elements +--------&gt; |filter+--&gt; |sorted+--&gt; |map+--&gt; |collect|+--------------------+ +------+ +------+ +---+ +-------+ 以上的流程转换为 Java 代码为：123456List&lt;Integer&gt; transactionsIds = widgets.stream() .filter(b -&gt; b.getColor() == RED) .sorted((x,y) -&gt; x.getWeight() - y.getWeight()) .mapToInt(Widget::getWeight) .sum(); Stream特性首先，Stream（流）是一个来自数据源的元素队列并支持聚合操作 元素是特定类型的对象，形成一个队列。 Java中的Stream并不会存储元素，而是按需计算。 数据源 流的来源。 可以是集合，数组，I/O channel， 产生器generator 等。 聚合操作 类似SQL语句一样的操作， 比如filter, map, reduce, find, match, sorted等。 和以前的Collection操作不同， Stream操作还有两个基础的特征： Pipelining: 中间操作都会返回流对象本身。 这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行(laziness)和短路( short-circuiting)。 内部迭代： 以前对集合遍历都是通过Iterator或者For-Each的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Stream提供了内部迭代的方式， 通过访问者模式(Visitor)实现。 生成流在 Java 8 中, 集合接口有两个方法来生成流： stream() − 为集合创建串行流。 串行流就是把内容整体遍历依次的逐个处理。不涉及到多线程问题。 parallelStream() − 为集合创建并行流。 并行流就是把内容分割成多个数据块，每个数据块对应一个流，然后用多个线程分别处理每个数据块中的流。 使用parallelStream()方法可以得到一个并行流，并行流底层使用的是forkjoin框架，对于一些计算量比较大的任务，使用并行流可能极大的提升效率。 Fork/Join框架 将一个大任务进行拆分Fork，拆分成若干个小任务（拆到不可再拆时），再将若干个小任务的计算结果进行Join汇总。 ForkJoin框架采用的是“工作窃取模式”，传统线程在处理任务时，假设有一个大任务被分解成了20个小任务，并由四个线程A,B,C,D处理，理论上来讲一个线程处理5个任务，每个线程的任务都放在一个队列中，当B,C,D的任务都处理完了，而A因为某些原因阻塞在了第二个小任务上，那么B,C,D都需要等待A处理完成，此时A处理完第二个任务后还有三个任务需要处理，可想而知，这样CPU的利用率很低。而ForkJoin采取的模式是，当B,C,D都处理完了，而A还阻塞在第二个任务时，B会从A的任务队列的末尾偷取一个任务过来自己处理，C和D也会从A的任务队列的末尾偷一个任务，这样就相当于B,C,D额外帮A分担了一些任务，提高了CPU的利用率。 12List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); 实例 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187import java.util.ArrayList;import java.util.Arrays;import java.util.IntSummaryStatistics;import java.util.List;import java.util.Random;import java.util.stream.Collectors;import java.util.Map; public class Java8Tester &#123; public static void main(String args[])&#123; System.out.println("使用 Java 7: "); // 计算空字符串 List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl"); System.out.println("列表: " +strings); long count = getCountEmptyStringUsingJava7(strings); System.out.println("空字符数量为: " + count); count = getCountLength3UsingJava7(strings); System.out.println("字符串长度为 3 的数量为: " + count); // 删除空字符串 List&lt;String&gt; filtered = deleteEmptyStringsUsingJava7(strings); System.out.println("筛选后的列表: " + filtered); // 删除空字符串，并使用逗号把它们合并起来 String mergedString = getMergedStringUsingJava7(strings,", "); System.out.println("合并字符串: " + mergedString); List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); // 获取列表元素平方数 List&lt;Integer&gt; squaresList = getSquares(numbers); System.out.println("平方数列表: " + squaresList); List&lt;Integer&gt; integers = Arrays.asList(1,2,13,4,15,6,17,8,19); System.out.println("列表: " +integers); System.out.println("列表中最大的数 : " + getMax(integers)); System.out.println("列表中最小的数 : " + getMin(integers)); System.out.println("所有数之和 : " + getSum(integers)); System.out.println("平均数 : " + getAverage(integers)); System.out.println("随机数: "); // 输出10个随机数 Random random = new Random(); for(int i=0; i &lt; 10; i++)&#123; System.out.println(random.nextInt()); &#125; System.out.println("使用 Java 8: "); System.out.println("列表: " +strings); count = strings.stream().filter(string-&gt;string.isEmpty()).count(); System.out.println("空字符串数量为: " + count); count = strings.stream().filter(string -&gt; string.length() == 3).count(); System.out.println("字符串长度为 3 的数量为: " + count); filtered = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.toList()); System.out.println("筛选后的列表: " + filtered); mergedString = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.joining(", ")); System.out.println("合并字符串: " + mergedString); squaresList = numbers.stream().map( i -&gt;i*i).distinct().collect(Collectors.toList()); System.out.println("Squares List: " + squaresList); System.out.println("列表: " +integers); IntSummaryStatistics stats = integers.stream().mapToInt((x) -&gt;x).summaryStatistics(); System.out.println("列表中最大的数 : " + stats.getMax()); System.out.println("列表中最小的数 : " + stats.getMin()); System.out.println("所有数之和 : " + stats.getSum()); System.out.println("平均数 : " + stats.getAverage()); System.out.println("随机数: "); random.ints().limit(10).sorted().forEach(System.out::println); // 并行处理 count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); System.out.println("空字符串的数量为: " + count); &#125; private static int getCountEmptyStringUsingJava7(List&lt;String&gt; strings)&#123; int count = 0; for(String string: strings)&#123; if(string.isEmpty())&#123; count++; &#125; &#125; return count; &#125; private static int getCountLength3UsingJava7(List&lt;String&gt; strings)&#123; int count = 0; for(String string: strings)&#123; if(string.length() == 3)&#123; count++; &#125; &#125; return count; &#125; private static List&lt;String&gt; deleteEmptyStringsUsingJava7(List&lt;String&gt; strings)&#123; List&lt;String&gt; filteredList = new ArrayList&lt;String&gt;(); for(String string: strings)&#123; if(!string.isEmpty())&#123; filteredList.add(string); &#125; &#125; return filteredList; &#125; private static String getMergedStringUsingJava7(List&lt;String&gt; strings, String separator)&#123; StringBuilder stringBuilder = new StringBuilder(); for(String string: strings)&#123; if(!string.isEmpty())&#123; stringBuilder.append(string); stringBuilder.append(separator); &#125; &#125; String mergedString = stringBuilder.toString(); return mergedString.substring(0, mergedString.length()-2); &#125; private static List&lt;Integer&gt; getSquares(List&lt;Integer&gt; numbers)&#123; List&lt;Integer&gt; squaresList = new ArrayList&lt;Integer&gt;(); for(Integer number: numbers)&#123; Integer square = new Integer(number.intValue() * number.intValue()); if(!squaresList.contains(square))&#123; squaresList.add(square); &#125; &#125; return squaresList; &#125; private static int getMax(List&lt;Integer&gt; numbers)&#123; int max = numbers.get(0); for(int i=1;i &lt; numbers.size();i++)&#123; Integer number = numbers.get(i); if(number.intValue() &gt; max)&#123; max = number.intValue(); &#125; &#125; return max; &#125; private static int getMin(List&lt;Integer&gt; numbers)&#123; int min = numbers.get(0); for(int i=1;i &lt; numbers.size();i++)&#123; Integer number = numbers.get(i); if(number.intValue() &lt; min)&#123; min = number.intValue(); &#125; &#125; return min; &#125; private static int getSum(List numbers)&#123; int sum = (int)(numbers.get(0)); for(int i=1;i &lt; numbers.size();i++)&#123; sum += (int)numbers.get(i); &#125; return sum; &#125; private static int getAverage(List&lt;Integer&gt; numbers)&#123; return getSum(numbers) / numbers.size(); &#125;&#125; 输出结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647使用 Java 7: 列表: [abc, , bc, efg, abcd, , jkl]空字符数量为: 2字符串长度为 3 的数量为: 3筛选后的列表: [abc, bc, efg, abcd, jkl]合并字符串: abc, bc, efg, abcd, jkl平方数列表: [9, 4, 49, 25]列表: [1, 2, 13, 4, 15, 6, 17, 8, 19]列表中最大的数 : 19列表中最小的数 : 1所有数之和 : 85平均数 : 9随机数: -393170844-963842252447036679-1043163142-881079698221586850-1101570113576190039-10451845781647841045使用 Java 8: 列表: [abc, , bc, efg, abcd, , jkl]空字符串数量为: 2字符串长度为 3 的数量为: 3筛选后的列表: [abc, bc, efg, abcd, jkl]合并字符串: abc, bc, efg, abcd, jklSquares List: [9, 4, 49, 25]列表: [1, 2, 13, 4, 15, 6, 17, 8, 19]列表中最大的数 : 19列表中最小的数 : 1所有数之和 : 85平均数 : 9.444444444444445随机数: -1743813696-1301974944-1299484995-7799811861365449025557920231243315896126492084914720771351706423674空字符串的数量为: 2 Nashorn JavaScriptNashorn [næʃən] 一个 javascript 引擎。从JDK 1.8开始，Nashorn取代Rhino(JDK 1.6, JDK1.7)成为Java的嵌入式JavaScript引擎。Nashorn完全支持ECMAScript 5.1规范以及一些扩展。它使用基于JSR 292的新语言特性，其中包含在JDK 7中引入的 invokedynamic，将JavaScript编译成Java字节码。与先前的Rhino实现相比，这带来了2到10倍的性能提升。 Java 中调用 JavaScript 123456789101112131415161718192021222324import javax.script.ScriptEngineManager;import javax.script.ScriptEngine;import javax.script.ScriptException; public class Java8Tester &#123; public static void main(String args[])&#123; ScriptEngineManager scriptEngineManager = new ScriptEngineManager(); ScriptEngine nashorn = scriptEngineManager.getEngineByName("nashorn"); String name = "Runoob"; Integer result = null; try &#123; nashorn.eval("print('" + name + "')"); result = (Integer) nashorn.eval("10 + 2"); &#125;catch(ScriptException e)&#123; System.out.println("执行脚本错误: "+ e.getMessage()); &#125; System.out.println(result.toString()); &#125;&#125; 输出结果 12Runoob12 日期时间 APIJava 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。 在旧版的 Java 中，日期时间 API 存在诸多问题，其中有： 非线程安全 − java.util.Date 是非线程安全的，所有的日期类都是可变的，这是Java日期类最大的问题之一。 设计很差 − Java的日期/时间类的定义并不一致，在java.util和java.sql的包中都有日期类，此外用于格式化和解析的类在java.text包中定义。java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期，将其纳入java.sql包并不合理。另外这两个类都有相同的名字，这本身就是一个非常糟糕的设计。 时区处理麻烦 − 日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。 Java 8 在 java.time 包下提供了很多新的 API。以下为两个比较重要的 API： Local(本地) − 简化了日期时间的处理，没有时区的问题。 Zoned(时区) − 通过制定的时区处理日期时间。 新的java.time包涵盖了所有处理日期，时间，日期/时间，时区，时刻（instants），过程（during）与时钟（clock）的操作。 实例 代码 123456789101112// 获取当前的日期时间 LocalDateTime currentTime = LocalDateTime.now(); System.out.println("当前时间: " + currentTime); LocalDate date1 = currentTime.toLocalDate(); System.out.println("date1: " + date1); Month month = currentTime.getMonth(); int day = currentTime.getDayOfMonth(); int seconds = currentTime.getSecond(); System.out.println("月: " + month +", 日: " + day +", 秒: " + seconds); 输出结果 当前时间: 2016-04-15T16:55:48.668 date1: 2016-04-15 月: APRIL, 日: 15, 秒: 48 参考文献Java 8 新特性]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Lambda</tag>
        <tag>Stream</tag>
        <tag>Optional</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL详解&优化记录]]></title>
    <url>%2F2019%2F02%2F14%2FMysql%E4%BC%98%E5%8C%96%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[概述MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，目前属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件。MySQL是一种关系数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。MySQL所使用的 SQL 语言是用于访问数据库的最常用标准化语言。MySQL 软件采用了双授权政策，分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择 MySQL 作为网站数据库。 新特性 表和索引的分区 行级复制 MySQL 基群基于磁盘的数据支持 MySQL 集群复制 增强的全文本搜索函数 增强的信息模式(数据字典) 可插入的 API 服务器日志表 XML（标准通用标记语言的子集）/ XPath支持 实例管理器 表空间备份 mysql_upgrade 升级程序 内部任务/事件调度器 新的性能工具和选项如 mysqlslap DBMS数据库管理系统(DataBase Manage System)。 DQL 数据查询语言（Data Query Language, DQL）是SQL语言中，负责进行数据查询而不会对数据本身进行修改的语句，这是最基本的SQL语句。 DDL 数据定义语言（Data Definition Language，DDL）是SQL语言集中负责数据结构定义与数据库对象定义的语言，由CREATE、ALTER与DROP三个语法所组成，最早是由Codasyl（Conference on Data Systems Languages）数据模型开始，现在被纳入SQL指令中作为其中一个子集。 DML 数据操纵语言（Data Manipulation Language, DML）欧美地区的开发人员把这四种指令，以“CRUD”(分别为 Create, Read, Update, Delete英文四前缀字母缩略的术语)来称呼；而亚洲地区使用汉语的开发人员，或可能以四个汉字：增 查 改 删 来略称。 DCL 数据控制语言 (Data Control Language) 在SQL语言中，是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。由 GRANT 和 REVOKE 两个指令组成。 MVCCMVCC 是一种多版本并发控制机制，是通过保存数据在某个时间点的快照来实现的. 不同存储引擎的MVCC. 不同存储引擎的MVCC实现是不同的,典型的有乐观并发控制和悲观并发控制。 大多数的MYSQL事务型存储引擎,如InnoDB，Falcon以及PBXT都不使用一种简单的行锁机制.事实上,他们都和MVCC–多版本并发控制来一起使用. 众所周知,锁机制可以控制并发操作,但是其系统开销较大,而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销. 事务隔离级别 事务是数据库系统区别于其他一切文件系统的重要特性之一 事务是一组具有原子性的SQL语句，或者一个独立的工作单元 mysql默认的事务隔离级别为 repeatable-read，可以使用如下命令查询：1select @@tx_isolation; 事务的基本要素（ACID） 1、原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 2、一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 3、隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 4、持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 事务的并发问题 1、脏读：未提交读(READ UNCOMMITED)，事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据。 2、不可重复读：已提交读(READ COMMITED)符合隔离性的基本概念，事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 3、可重复读：(REPEATABLE READ) InnoDB的默认隔离等级。事务进行时，其它所有事务对其不可见，即多次执行读，得到的结果是一样的！ 4、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 数据库锁MySQL中锁主要作用是针对共享资源的并发访问以及用于实现事务的隔离性。 MySQL数据中常见的锁解读 乐观锁乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。乐观事务控制最早是由孔祥重（H.T.Kung）教授提出。 乐观并发控制多数用于数据争用不大、冲突较少的环境中，这种环境中，偶尔回滚事务的成本会低于读取数据时锁定数据的成本，因此可以获得比其他并发控制方法更高的吞吐量。 优缺点： 乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。 悲观锁 悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作读某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。 悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 按使用性质划分 共享锁（Share Lock） S锁，也叫读锁，用于所有的只读数据操作。共享锁是非独占的，允许多个并发事务读取其锁定的资源。 多个事务可封锁同一个共享页； 任何事务都不能修改该页； 通常是该页被读取完毕，S锁立即被释放。 更新锁（Update Lock） U锁，在修改操作的初始化阶段用来锁定可能要被修改的资源，这样可以避免使用共享锁造成的死锁现象。因为当使用共享锁时，修改数据的操作分为两步： 首先获得一个共享锁，读取数据，然后将共享锁升级为排他锁，再执行修改操作。 这样如果有两个或多个事务同时对一个事务申请了共享锁，在修改数据时，这些事务都要将共享锁升级为排 他锁。这时，这些事务都不会释放共享锁，而是一直等待对方释放，这样就造成了死锁。 如果一个数据在修改前直接申请更新锁，在数据修改时再升级为排他锁，就可以避免死锁。 排他锁（Exclusive Lock） X锁，也叫写锁，表示对数据进行写操作。如果一个事务对对象加了排他锁，其他事务就不能再给它加任何锁了。（某个顾客把试衣间从里面反锁了，其他顾客想要使用这个试衣间，就只有等待锁从里面打开了。） 按作用范围划分 行锁 锁的作用范围是行级别。 表锁 锁的作用范围是整张表。数据库能够确定那些行需要锁的情况下使用行锁，如果不知道会影响哪些行的时候就会使用表锁。 页锁 对表中一组连续的行进行加锁。 性能比较 表锁：开销小，加锁快；不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低 行锁：开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 页锁：开销和加锁速度介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般 优缺点： 悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数据。 MySQL性能影响MySQL性能的几个因素： QPS：Queries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数。 TPS：是TransactionsPerSecond的缩写，也就是事务数/秒。它是软件测试结果的测量单位。 服务器硬件 服务器系统（系统参数优化） 磁盘IO 磁盘IO性能突然下降、大量消耗磁盘性能的计划任务。解决：更快磁盘设备、调整计划任务、做好磁盘维护。 网络延迟 存储引擎。 MyISAM： 不支持事务，表级锁。 InnoDB: 支持事务，支持行级锁，事务ACID。 数据库参数配置 数据库结构设计和SQL语句。（重点优化） 默认库解读 information_schema information_schema提供了访问数据库元数据的方式。(元数据是关于数据的数据，如数据库名或表名，列的数据类型，或访问权限等。有时用于表述该信息的其他术语包括“数据词典”和“系统目录”。) 换句换说，information_schema是一个信息数据库，它保存着关于MySQL服务器所维护的所有其他数据库的信息。(如数据库名，数据库的表，表栏的数据类型与访问权 限等。) 在INFORMATION_SCHEMA中，有几张只读表。它们实际上是视图，而不是基本表。 mysql mysql的核心数据库，类似于sql server中的master表，主要负责存储数据库的用户、权限设置、关键字等mysql自己需要使用的控制和管理信息。(常用的，在mysql.user表中修改root用户的密码)。 performance_schema 主要用于收集数据库服务器性能参数。并且库里表的存储引擎均为PERFORMANCE_SCHEMA，而用户是不能创建存储引擎为PERFORMANCE_SCHEMA的表。MySQL5.7默认是开启的。 sys Sys库所有的数据源来自：performance_schema。目标是把performance_schema的把复杂度降低，让DBA能更好的阅读这个库里的内容。让DBA更快的了解DB的运行情况。 体系结构 客户端 服务层 连接管理、查询缓存、查询解析、查询优化器 存储引擎 执行流程 存储引擎MySQL常见的存储引擎有InnoDB和MyISAM两种，其他NDBCluster、Merge、Memory等稍微了解即可。 InnoDB 在InnoDB中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。 InnoDB 使用 B+Tree 作为索引结构，叶节点直接存储的是完整的数据记录，索引的 key 是数据表的主键,因此 InnoDB 表数据文件本身就是主索引。 查看InnoDB数据存储 1show variables like 'innodb_file_per_table' 如果innodb_file_per_table 为 ON 将建立独立的表空间，文件为tablename.ibd； 如果innodb_file_per_table 为 OFF 将数据存储到系统的共享表空间，文件为ibdataX（X为从1开始的整数）； MySQL5.5默认系统表空间与MySQL5.6及以后默认独立表空间 1.系统表空间无法简单的收缩文件大小，造成空间浪费，并会产生大量的磁盘碎片。 2.独立表空间可以通过optimeze table 收缩系统文件，不需要重启服务器也不会影响对表的正常访问。 3.如果对多个表进行刷新时，实际上是顺序进行的，会产生IO瓶颈。 4 独立表空间可以同时向多个文件刷新数据。 5 强烈建立对Innodb 使用独立表空间，优化什么的更方便，可控。 特性：支持事务和行级锁（具体说明参考事务和锁） InnoDB的存储文件格式分为： .frm:表结构定义信息 .ibd:表数据和索引数据 MyISAM MyISAM存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的key都存储指向键值对应的数据的物理地址。 非聚簇索引的数据表和索引表是分开存储的。 非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。 只有在MyISAM中才能使用FULLTEXT索引。 MyISAM 引擎默认使用 B+Tree 作为索引结构,叶节点的 data 域存放的是数据记录的地址。 MyISAM存储引擎的表在数据库中，每一个表都被存放为三个以表名命名的物理文件。首先肯定会有任何存储引擎都不可缺少的存放表结构定义信息的.frm文件，另外还有.MYD和.MYI文件，分别存放了表的数据（.MYD）和索引数据（.MYI）。每个表都有且仅有这样三个文件做为MyISAM存储类型的表的存储，也就是说不管这个表有多少个索引，都是存放在同一个.MYI文件中。 支持R-Tree索引、R-Tree索引、全文索引。 MyISAM的存储文件格式分为： .frm:表结构定义信息 .MYD:表的数据 .MYI:索引数据 InnoDB和MyISAM的区别 使用主索引的时候，更适合使用聚簇索引，因为聚簇索引只需要查找一次，而非聚簇索引在查到数据的地址后，还要进行一次I/O查找数据。 因为聚簇辅助索引存储的是主键的键值，因此可以在数据行移动或者页分裂的时候降低委会成本，因为这时不用维护辅助索引。但是辅助索引会占用更多的空间。 聚簇索引在插入新数据的时候比非聚簇索引慢很多，因为插入新数据时需要减压主键是否重复，这需要遍历主索引的所有叶节点，而非聚簇索引的叶节点保存的是数据地址，占用空间少，因此分布集中，查询的时候I/O更少，但聚簇索引的主索引中存储的是数据本身，数据占用空间大，分布范围更大，可能占用好多的扇区，因此需要更多次I/O才能遍历完毕。 Archive Archive非常适合存储大量的独立的，作为历史记录的数据。因为它们不经常被读取。Archive 拥有高效的插入速度，但其对查询的支持相对较差。 Memory Memory所有数据置于内存的存储引擎，拥有极高的插入，更新和查询效率。但是会占用和数据量成正比的内存空间。并且其内容会在 MySQL 重新启动时丢失 Merge Merge是将一定数量的 MyISAM 表联合而成一个整体，在超大规模数据存储时很有用。 CSV 逻辑上由逗号分割数据的存储引擎。它会在数据库子目录里为每个数据表创建一个 .csv 文件。这是一种普通文本文件，每个数据行占用一个文本行。CSV 存储引擎不支持索引。 BlackHole 黑洞引擎，写入的任何数据都会消失，一般用于记录 binlog 做复制的中继。 EXAMPLE EXAMPLE 存储引擎是一个不做任何事情的存根引擎。它的目的是作为 MySQL 源代码中的一个例子，用来演示如何开始编写一个新存储引擎。同样，它的主要兴趣是对开发者。EXAMPLE 存储引擎不支持编索引。 数据预热默认情况，仅仅有某条数据被读取一次，才会缓存在innodb_buffer_pool。所以，数据库刚刚启动，须要进行数据预热，将磁盘上的全部数据缓存到内存中。数据预热能够提高读取速度。对于InnoDB数据库，能够用下面方法，进行数据预热。 1.将下面脚本保存为 MakeSelectQueriesToLoad.sql 123456789101112131415161718192021222324252627SELECT DISTINCT CONCAT('SELECT ',ndxcollist,' FROM ',db,'.',tb, ' ORDER BY ',ndxcollist,';') SelectQueryToLoadCache FROM ( SELECT engine,table_schema db,table_name tb, index_name,GROUP_CONCAT(column_name ORDER BY seq_in_index) ndxcollist FROM ( SELECT B.engine,A.table_schema,A.table_name, A.index_name,A.column_name,A.seq_in_index FROM information_schema.statistics A INNER JOIN ( SELECT engine,table_schema,table_name FROM information_schema.tables WHERE engine='InnoDB' ) B USING (table_schema,table_name) WHERE B.table_schema NOT IN ('information_schema','mysql') ORDER BY table_schema,table_name,index_name,seq_in_index ) A GROUP BY table_schema,table_name,index_name ) AAORDER BY db,tb; 2.运行 1mysql -uroot -AN &lt; /root/MakeSelectQueriesToLoad.sql &gt; /root/SelectQueriesToLoad.sql 3.加入启动脚本到 my.cnfbashvim /etc/my.cnf [mysqld] init-file=/var/lib/mysql/upcache.sql1234567### MySQL执行线程**1、查看`MySQL`当前连接数**```bashshow full processlist; statistics 状态的线程，表示正在计算统计数据，以制定一个查询执行计划。 如果一个线程很长一段时间处于这种状态，可能是磁盘绑定的，即磁盘IO性能很差，说到底就是数据库服务器IO遇到问题了，可以通过增加 buffer_pool 来缓存更多的数据，或者提高服务器IO能力，可能磁盘在执行其他工作。 服务器IO状态会一直占用cpu，导致性能严重下降。 2、显示MySQL状态 1show status; Aborted_clients 由于客户没有正确关闭连接已经死掉，已经放弃的连接数量。 Aborted_connects 尝试已经失败的MySQL服务器的连接的次数。 Connections 试图连接MySQL服务器的次数。 Created_tmp_tables 当执行语句时，已经被创造了的隐含临时表的数量。 Delayed_insert_threads 正在使用的延迟插入处理器线程的数量。 Open_tables 打开表的数量。 Open_files 打开文件的数量。 Open_streams 打开流的数量(主要用于日志记载） Opened_tables 已经打开的表的数量。 Questions 发往服务器的查询的数量。 Slow_queries 要花超过long_query_time时间的查询数量。 Threads_connected 当前打开的连接的数量。 Threads_running 不在睡眠的线程数量。 Uptime 服务器工作了多少秒。 3、终止正在执行的MySQL线程 1kill id; 杀死耗时或者长时间处于Sending to Client状态的慢sql。 减少磁盘绑定： 1) 扩大innodb_buffer_pool_size的缓冲池大小。如果你用Innodb然后，当表数据缓存在InnoDB缓冲池中时，可以通过查询一次又一次地处理表数据，而不需要任何磁盘I/O。这个内存区域非常重要，以至于繁忙的数据库通常指定的大小约为物理内存量的80%。 2) 在GNU/Linux和Unix的某些版本中，使用Unixfsync()调用(InnoDB默认使用)将文件刷新到磁盘，类似的方法非常慢。如果数据库写入性能是一个问题，则使用INNODB_FLUSH_Method参数设置为O_DSYNC进行基准测试。 3) 扩大innodb_buffer_pool_size的数据库日志缓冲区大小-设置分配给存储InnoDB预写日志条目的缓冲区的内存量。对于大型事务，可以将日志加载到日志缓冲区中，而不是将日志写入磁盘上的日志文件，直到日志缓冲区在每次事务提交时被刷新为止。如果您在运行时在显示nodb状态输出中看到大型日志I/O，则可能需要为innodb_log_buffer_size参数设置更大的值，以保存磁盘I/O。 4) 增加用于缓存表和查询的内存-检查表和查询的缓存命中率，检查并增加这些MySQL变量：query_cache_size, query_cache_limit, query_cache_min_res_unit, tmp_table_size, join_buffer_size , sort_buffer_size等等。 5) 确保对服务器上的所有表都应用了适当的索引，并使用正确的数据类型。 建议：查看当前io性能状态，例如iostat SQL调优 SQL执行顺序 12345678910FROM &lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN &lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECTDISTINCT &lt;select_list&gt;ORDER BY &lt;order_by_condition&gt;LIMIT &lt;limit_number&gt; SQL优化 禁用select * 使用select count(*) 统计行数 尽量少运算 尽量避免全表扫描，如果可以，在过滤列建立索引 尽量避免在where子句对字段进行null判断 尽量避免在where子句使用!= 或者&lt;&gt; 尽量避免在where子句使用or连接 尽量避免对字段进行表达式计算 尽量避免对字段进行函数操作 尽量避免使用不是复合索引的前缀列进行过滤连接 尽量少排序，如果可以，建立索引 尽量少join 尽量用join代替子查询 尽量避免在where子句中使用in,not in或者having，使用exists,not exists代替 尽量避免两端模糊匹配 like %*% 尽量用union all代替union 尽量早过滤 避免类型转换 尽量批量insert 优先优化高并发sql，而不是频率低的大sql 尽可能对每一条sql进行explain 尽可能从全局出发 Order By优化 filesort优化算法. 在mysql version()&lt;4.1之前，优化器采用的是filesort第一种优化算法，先提取键值和指针，排序后再去提取数据，前后要搜索数据两次，第一次若能使用索引则使用，第二次是随机读(当然不同引擎也不同)。mysql version()&gt;=4.1,更新了一个新算法，就是在第一次读的时候也把selcet的列也读出来，然后在sort_buffer_size中排序(不够大则建临时表保存排序顺序)，这算法只需要一次读取数据。所以有这个广为人传的一个优化方法，那就是增大sort_buffer_size。Filesort第二种算法要用到更的空间，sort_buffer_size不够大反而会影响速度，所以mysql开发团队定了个变量max_length_for_sort_data，当算法中读出来的需要列的数据的大小超过该变量的值才使用，所以一般性能分析的时候会尝试把max_length_for_sort_data改小。 单独order by 用不了索引，索引考虑加where 或加limit。 当order by 和 group by无法使用索引时，增大max_length_for_sort_data参数设置和增大sort_buffer_size参数的设置。 索引命中&amp;失效索引是一种特殊的文件（InnoDB 数据表上的索引是表空间的一个组成部分），它们包含着对数据表里所有记录的引用指针。索引不是万能的，索引可以加快数据检索操作，但会使数据修改操作变慢。每修改数据记录，索引就必须刷新一次。为了在某种程度上弥补这一缺陷，许多 SQL 命令都有一个 DELAY_KEY_WRITE 项。这个选项的作用是暂时制止 MySQL 在该命令每插入一条新记录和每修改一条现有之后立刻对索引进行刷新，对索引的刷新将等到全部记录插入/修改完毕之后再进行。在需要把许多新记录插入某个数据表的场合，DELAY_KEY_WRITE 选项的作用将非常明显。另外，索引还会在硬盘上占用相当大的空间。因此应该只为最经常查询和最经常排序的数据列建立索引。注意，如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果。 数据库表的索引从数据存储方式上可以分为聚簇索引和非聚簇索引（又叫二级索引）两种。 聚簇索引 Innodb的聚簇索引在同一个B-Tree中保存了索引列和具体的数据，在聚簇索引中，实际的数据保存在叶子页中，中间的节点页保存指向下一层页面的指针。“聚簇”的意思是数据行被按照一定顺序一个个紧密地排列在一起存储。一个表只能有一个聚簇索引，因为在一个表中数据的存放方式只有一种。 聚簇索引的优点 聚簇索引将索引和数据行保存在同一个B-Tree中，查询通过聚簇索引可以直接获取数据，相比非聚簇索引需要 第二次查询（非覆盖索引的情况下）效率要高。 聚簇索引对于范围查询的效率很高，因为其数据是按照大小排列的， 聚簇索引的缺点 聚簇索引的更新代价比较高，如果更新了行的聚簇索引列，就需要将数据移动到相应的位置。这可能因为要插 入的页已满而导致“页分裂”。 插入速度严重依赖于插入顺序，按照主键进行插入的速度是加载数据到Innodb中的最快方式。如果不是按照主 键插入，最好在加载完成后使用OPTIMIZE TABLE命令重新组织一下表。 聚簇索引在插入新行和更新主键时，可能导致“页分裂”问题。 聚簇索引可能导致全表扫描速度变慢，因为可能需要加载物理上相隔较远的页到内存中（需要耗时的磁盘寻道操作）。 非聚簇索引 非聚簇索引，又叫二级索引。二级索引的叶子节点中保存的不是指向行的物理指针，而是行的主键值。当通过二级索引查找行，存储引擎需要在二级索引中找到相应的叶子节点，获得行的主键值，然后使用主键去聚簇索引中查找数据行，这需要两次B-Tree查找。 普通索引 普通索引（由关键字 KEY 或 INDEX 定义的索引）的任务是加快对数据的访问速度。因此，应该只为那些最经常出现查询条件（WHERE column =）或排序条件（ORDER BY column）中的数据列创建索引。只要有可能，就应该选择一个数据最整齐、最紧凑的数据列（如一个整数类型的数据列）来创建索引。 唯一索引 与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。 主键索引 主键索引是一种特殊的唯一索引，不允许有空值，这个索引就是所谓的“主索引”。主索引区别是：前者在定义时使用的关键字是 PRIMARY 而不是 UNIQUE。 BTree索引 叶子结点指针都为null；非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据；BTree的结构下，就可以使用二分查找的查找方式，查找复杂度为h*log(n)，一般来说树的高度是很小的，一般为3左右，因此BTree是一个非常高效的查找结构。 B+Tree索引 B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：B+Tree中的非叶子结点不存储数据，只存储键值；B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应的数据的物理地址； 组合索引 遵循最左前缀原则，索引可以覆盖多个数据列，如像 INDEX (columnA, columnB) 索引。这种索引的特点是 MySQL 可以有选择地使用一个这样的索引。如果查询操作只需要用到 columnA 数据列上的一个索引，就可以使用复合索引 INDEX(columnA, columnB)。不过，这种用法仅适用于在复合索引中排列在前的数据列组合。比如说，INDEX (A，B，C) 可以当做 A 或 (A,B) 的索引来使用，但不能当做 B、C 或 (B,C) 的索引来使用。 全文索引 仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。 索引失效 关联字段类型不同索引字段使用函数表达式like %AAAis null GROUP BY 的索引应用 1、查询字段必须和后面GROUP BY 一致select TeamID from competeinfo where TeamID &gt;10 group by TeamID。这里就是通过TeamID 来查找。完成group by 。 2、联合索引的应用，切记注意GROUP BY 顺序，Where 条件和GROUP BY 字段得是一个索引里面的这个表CompeteID,TeamID建立联合索引1）select TeamID from competeinfo where TeamID &gt;10 and CompeteID &gt; 100020 group by CompeteID这个查询用到了CompeteID,TeamID联合索引2）select TeamID from competeinfo where TeamID &gt;10 and CompeteID &gt; 100020 group by TeamID这样的话查询group by中就没有用到索引了 下面是总结的是联合索引的使用： Index（Name，Age）表示在Name，Age两列上建立联合索引 如果where name=&apos;pp&apos; 能使用索引 where age=25时不能使用索引 where name=&apos;pp&apos; and age&gt;25 能使用索引 where name =&apos;pp&apos; order by age 能使用索引 where name&gt;&apos;pp&apos; order by age 不能使用索引 where name&gt;&apos;pp&apos; order by name,age 能使用索引 order by name asc age desc 将不能使用索引！ 主从配置文件MySQL主从复制时时同步，是在开启log-bin日志文件的基础上，开始的时候记录主库的Position，作为从库同步的开始节点,所以主从库都必须开启log-bin日志，恢复数据也是根据此文件来操作要恢复的节点。 查看主库Position： 主库配置12GRANT replication slave ON *.* TO 'root'@'10.10.253.%' identified by '123456';flush privileges; 主库配置文件123456789101112131415161718#安装目录basedir = /usr/local/mysql#数据目录datadir = /usr/local/mysql/data#端口port = 3306#socket 路径socket = /var/lib/mysql/mysql.sock#log-bin日志文件log-bin=mysql-bin-1#serverId，一般为ip的最后一段，避免重复server_id=119#开启查询缓存explicit_defaults_for_timestamp=true#设置最新日志保留时间，节省磁盘空间expire_logs_days = 10#sql_mode模式，定义了mysql应该支持的sql语法，数据校验等sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES sql_mode常见的sql_mode有以下几种 NO_ENGINE_SUBSTITUTION模式 如果需要的存储引擎被禁用或未编译，那么抛出错误。不设置此值时，用默认的存储引擎替代，并抛出一个异常。STRICT_TRANS_TABLES模式 严格模式，进行数据的严格校验，错误数据不能插入，报error错误。在该模式下，如果一个值不能插入到一个事务表中，则中断当前的操作，对非事务表不做限制。ANSI模式 宽松模式，对插入数据进行校验，如果不符合定义类型或长度，对数据类型调整或截断保存，报warning警告。 TRADITIONAL模式 严格模式，当向mysql数据库插入数据时，进行数据的严格校验，保证错误数据不能插入，报error错误。用于事物时，会进行事物的回滚。 ONLY_FULL_GROUP_BY模式 出现在select语句、HAVING条件和ORDER BY语句中的列，必须是GROUP BY的列或者依赖于GROUP BY列的函数列。NO_AUTO_VALUE_ON_ZERO模式 该值影响自增长列的插入。默认设置下，插入0或NULL代表生成下一个自增长值。如果用户希望插入的值为0，而该列又是自增长的，那么这个选项就有用了。NO_ZERO_IN_DATE模式 这个模式影响了是否允许日期中的月份和日包含0。如果开启此模式，2016-01-00是不允许的，但是0000-02-01是允许的。它实际的行为受到 strict mode是否开启的影响1。NO_AUTO_CREATE_USER模式 禁止GRANT创建密码为空的用户ANSI_QUOTES模式 启用ANSI_QUOTES后，不能用双引号来引用字符串，因为它被解释为识别符。 从库设置 根据查看到的主库status，记录log-bin和position。 12change master to master_host='192.168.80.9',master_user='root',master_password='123456',master_log_file='mysql-bin-1.000003',master_log_pos=13177757 ; 从库配置文件123456789101112131415161718#安装目录basedir = /usr/local/mysql#数据目录datadir = /usr/local/mysql/data#端口port = 3306#socket 路径socket = /var/lib/mysql/mysql.sock#log-bin日志文件log-bin=mysql-bin-1#serverId，一般为ip的最后一段，避免重复server_id=119#开启查询缓存explicit_defaults_for_timestamp=true#设置需要同步主库的DBreplicate-do-db=hsg_oa#设置不需要同步主库的DBreplicate-ignore-db=information_schema,mysql,performance_schema,sys 删除主从库data下面的auto.cnf，会导致主从库同步节点不一致问题。 默认配置参数 sort_buffer_size 定义了每个线程排序缓存区的大小，MySQL在有查询、需要做排序操作时才会为每个缓冲区分配内存（直接分配该参数的全部内存）； join_buffer_size 定义了每个线程所使用的连接缓冲区的大小，如果一个查询关联了多张表，MySQL会为每张表分配一个连接缓冲，导致一个查询产生了多个连接缓冲； read_buffer_size 定义了当对一张MyISAM进行全表扫描时所分配读缓冲池大小，MySQL有查询需要时会为其分配内存，其必须是4k的倍数； read_rnd_buffer_size 索引缓冲区大小，MySQL有查询需要时会为其分配内存，只会分配需要的大小。 注意：以上四个参数是为一个线程分配的，如果有100个连接，那么需要×100。 MySQL数据库实例： MySQL是单进程多线程（而oracle是多进程），也就是说MySQL实例在系统上表现就是一个服务进程，即进程； MySQL实例是线程和内存组成，实例才是真正用于操作数据库文件的； 一般情况下一个实例操作一个或多个数据库；集群情况下多个实例操作一个或多个数据库。 如何为缓存池分配内存： Innodb_buffer_pool_size，定义了Innodb所使用缓存池的大小，对其性能十分重要，必须足够大，但是过大时，使得Innodb 关闭时候需要更多时间把脏页从缓冲池中刷新到磁盘中； 总内存-（每个线程所需要的内存*连接数）-系统保留内存 key_buffer_size，定义了MyISAM所使用的缓存池的大小，由于数据是依赖存储操作系统缓存的，所以要为操作系统预留更大的内存空间； select sum(index_length) from information_schema.talbes where engine=’myisam’ 注意：即使开发使用的表全部是Innodb表，也要为MyISAM预留内存，因为MySQL系统使用的表仍然是MyISAM表。max_connections 控制允许的最大连接数，一般2000更大。不要使用外键约束保证数据的完整性。 日志事务性存储引擎及主要日志类型：Redo Log 、 Undo Log 和 general log Redo Log：实现事务的持久性(已提交的事务)。 Undo Log：未提交的事务，独立于表空间，需要随机访问，可以存储在高性能io设备上 general log：mysql将对DB的所有操作都可以记录在general log中，我们可以在线开启也可以通过修改初始化参数开启，调试用。 主要说明一下general log的使用： 在线打开和关闭12set global general_log=on;set global general_log=off; 通过下面sql方法可以查看mysql.general_log表的存储引擎是什么show table status from mysql where name=’general_log’; 这个表的缺省存储引擎是CSV，我们可以将该存储引擎修改成myisam来达到更好的性能。1234567891011mysql&gt; alter table mysql.general_log engine=myisam;1580 - You cannot 'ALTER' a log table if logging is enabledmysql&gt; set global general_log=off;Query OK, 0 rows affectedmysql&gt; alter table mysql.general_log engine=myisam;Query OK, 11 rows affectedRecords: 11 Duplicates: 0 Warnings: 0mysql&gt; set global general_log=on;Query OK, 0 rows affected 记录general log可以很好的监控当前数据库中的所有操作，可以选择在忙时打开，这样就可以对性能进行优化。但general log比较消耗系统性能，生产系统不建议打开 视图视图概述 视图本身是一个虚拟表，不存放任何数据，查询视图的数据集由其他表生成。MySQL底层通过两种算法来实现视图：临时表算法（TEMPTABLE）和合并算法（MERGE）。 临时表算法就是将SELECT语句的结果存放到临时表中，当需要访问视图的时候，直接访问这个临时表即可。 相比于其它关系型数据库的视图，MySQL的视图在功能上会弱很多，比如ORACLE和MS SQL SERVER都支持物化视图。物化视图是指将视图结果数据存放在一个可以查询的表中，并定期从原始表中刷新数据到这张表中，这张表和普通物理表一样，可以创建索引、主键约束等等，性能相比于临时表会有质的提升。但遗憾的是MySQL目前并不支持物化视图，当然MySQL也不支持在视图中创建索引。 视图的实现算法是视图本身的属性决定的，跟作用在视图上的SQL没有任何关系。一般来说，只要原表记录和视图中的记录无法建立一一映射的关系时，MySQL都将使用临时表算法来实现视图。比如创建视图的SQL中包含GROUP BY、DISTINCT、UNION、聚合函数、子查询的时候，视图都将采用临时表算法（这些规则在以后的版本中，可能会发生改变，具体请参考官方手册） 视图用途 首先视图可以简化应用上层的操作，让应用更专注于其所关心的数据。其次，视图能够对敏感数据提供安全保护，比如：对不同的用户定义不同的视图，可以使敏感数据不出现在不应该看到这些数据的用户视图上；也可以使用视图实现基于列的权限控制，而不需要真正的在数据库中创建列权限。再者，视图可以方便系统运维，比如：在重构schema的时候使用视图，使得在修改视图底层表结构的时候，应用代码还可以继续运行不报错。 Explain&amp;Profiling Explain 使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是 如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。 使用方式：Explain+SQL语句 详细参数说明参见MySQL 性能优化神器 Explain 使用分析，里面写的很有特点。 Profiling 开启profiling设置 查看profiling信息状态: on为开启，off关闭 12show variables like '%profiling%'; set profiling=1; 执行SQL查询 12SELECT * FROM oa_sign_in ; show profiles; 1show profile cpu,block io for query 168; 参考文献 statistics State in MySQL Processlist List of background operations performed by MySQL InnoDB Main thread processlist中哪些状态要引起关注 是什么导致MySQL数据库服务器磁盘I/O高？ table_open_cache 与 table_definition_cache 对MySQL(内存)的影响 8.5.8 Optimizing InnoDB Disk I/O MySQL的四种事务隔离级别 轻松理解MYSQL MVCC 实现机制 MySQL性能管理及架构设计 MySQL索引背后的数据结构及算法原理]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>优化</tag>
        <tag>sql</tag>
        <tag>主从配置</tag>
        <tag>事务</tag>
        <tag>索引</tag>
        <tag>存储引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins自动化安装部署-权限配置（三）]]></title>
    <url>%2F2019%2F02%2F10%2FJenkins%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2-%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[权限配置Jenkins的权限配置很重要，尤其是项目团队比较多的时候，可以防止一些奇葩的程序员进行误操作，一般情况下，只有A项目组的人才可以构建和操作A项目的权限，其他项目组是看不到的。因此合理的配置Jenkins权限可以使其发挥更大的功效，避免操作额外的麻烦。 系统管理里面选择Manage and Assign Roles，来配置Jenkins的相关权限。 Manage Roles ：包括全局角色和项目角色 Assign Roles ：分配全局和项目角色 分配全局角色 分配项目角色 配置完成只有拥有Jenkins分配到权限的人才可以构建项目操作。 构建日志 项目构建历史里点击控制台输出即可查看当前项目构建的过程、更新了哪些文件以及服务器部署过程，便于快速的定位问题以及代码错误的文件名称。 在当前构建系统中可以查看到所有依赖的环境变量。 变更记录可以快速定位到某个开发提交了哪个错误文件导致的部署失败问题。 部署问题Tomcat应用更新时，把新的WAR包放到webapps目录下，Tomcat就会自动把原来的同名webapp删除，并把WAR包解压，运行新的 webapp。 有时候Tomcat并不能把旧的webapp完全删除，通常会留下WEB-INF/lib下的某个jar包，必须关闭Tomcat才能删除，这就导致自动部署失败。解决方法：查看tomcate/conf/context.xml中的 context节点，元素中增加一个属性antiResourceLocking=”true” antiJARLocking=”true”，默认是”false”。这样就可以热部署了。这两个参数就是配置Tomcat的资源锁定和Jar包锁定策略。 延伸问题，解压后的原context.xml文件中的context节点默认有两个属性：antiResourceLocking=”true” antiJARLocking=”true”，即Tomcat的资源锁定和Jar包锁定策略。 当antiResourceLocking=”true”时，tomcat每次启动时都会在temp目录下生成相应工程的临时文件，并把这些文件锁定，所以即使修改文件后，webapps文件夹下的修改时间会更新，但是temp文件夹下的修改时间是启动tomcate之前的修改时间。而用户在浏览器访问的资源却是temp下的，不是webapps下的。所以最终导致用户修改的效果无法在浏览器中看到。当antiResourceLocking=“false”后，temp目录下就不会生成临时文件了，这是用户访问的就是webapps下的资源了。 解决方法：查看tomcate/conf/context.xml中的 context节点。如果有antiResourceLocking这个属性，应该把他的值设为false。通过设置reloadable=”true” 属性，该属性主要用于自动部署java文件的修改。 1&lt;Context antiResourceLocking="true"&gt; 注：以上问题是直接通过tomcat内部账号连接部署的时候会发生的问题，如果直接通过scp文件上传打包到服务器不会出现上述问题。 如果部署后出现缓存问题，修改的配置文件等没有生效，手动删除Jenkins工作空间的war文件。 注：如果配置文件进行修改后，而已经部署到Jenkins的workspace中的war没有清理，继续构建的话，Jenkins会默认保留已构建的war，而不会将最新的文件关联到nexus中。 在jenkins中配置自动更新部署项目时，如果采取用execute shell启动/关闭tomcat，会发现可以进行关闭tomcat，但是无法启动tomcat，虽然构建会显示执行成功，但是查看进程，tomcat是没有启动的。这是因为Jenkins默认会在Build结束后Kill掉所有的衍生进程。需要进行以下配置，才能避免此类情况发生。 在启动jenkins 的时候禁止jenkins杀死衍生进程 修改/etc/sysconfig/jenkins配置，在JENKINS_JAVA_OPTIONS中加入-Dhudson.util.ProcessTree.disable=true。需要重启jenkins生效 此方法配置一次后，所有的job都无需设置BUILD_ID，就能够防止jenkins杀死启动的tomcat进]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>权限</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins自动化安装部署-环境配置（二）]]></title>
    <url>%2F2019%2F02%2F10%2FJenkins%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[经过上述安装步骤，Jenkins已经成功安装到所在服务器中，下面通过访问地址进入web页面操作。 打开浏览器输出：http://服务器IP地址:8888/ 打开url会出现一个配置Jenkins的登录密码的界面，设置一个密码，继续进入如下面界，选择建议安装的插件，进行安装，等待安装完毕。 安装成功之后，进入Jenkins的主页面，开始新建job，也就是所谓的项目名，Jenkins中的项目名字相当于实际项目的逻辑名称，与之对应存储在jenkins的workspaces下。 Global Tool Configuration 在系统管理-全局环境配置，可以配置系统所需要的JDK、ANT、GIT、MAVEN、GRADLE、DOCKER等环境变量。 其次在项目配置中的源码管理里选择当前项目的svn资源库相关配置。以便部署项目的时候直接会从SVN服务器更新编译到本地。 注：一定要加@HEAD是把你本机工作区的版本更新到服务器上最新版本，这样可以避免造成文件冲突导致的文件不可用问题。 在构建中选择方式：Invoke Ant，并选择之前配置的ant版本，此处不能为default。 可以通过编辑视图来添加相关文档说明，也可以复制启动Banner直接到视图里。 配置完成之后在项目列表就可以针对当前项目进行构建自动化部署操作。 至此基本环境配置和项目配置就结束了，开始配置并分配用户权限以及项目开发人员的访问权限。]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Jenkins自动化安装部署-安装配置（一）]]></title>
    <url>%2F2019%2F02%2F10%2FJenkins%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Jenkins概述Jenkins是一个独立的开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。前身是Hudson是一个可扩展的持续集成引擎。可用于自动化各种任务，如构建，测试和部署软件。Jenkins可以通过本机系统包Docker安装，甚至可以通过安装Java Runtime Environment的任何机器独立运行。 Jenkins特点 开源免费; 跨平台，支持所有的平台; master/slave支持分布式的build; web形式的可视化的管理页面; 安装配置超级简单; tips及时快速的帮助; 已有的200多个插件 系统要求 环境 说明 配置环境 JDK JDK1.5以上 JDK1.7 硬盘 无最低要求、根据实际项目大小适当即可。 80G 操作系统 Jenkins可以安装到主流的所有操作系统。 CentOS Java容器 WAR文件可以在支持 Servlet2.4/JSP2.0或更高版本的容器中运行 独立安装 代码库 支持SVN或者GIT等同步插件 （示例svn） svn 编译打包 支持Maven或ANT来自动构建（示例ant） ant 需求概述由于项目架构是SpringMVC，并没有使用Maven来构建，所以本文安装的Jenkins基于Ant环境构建发布（也可以使用Maven），配置比较方便。 Ant安装1[root@localhost jenkins]# vi /etc/profile 在最后添加 123ANT_HOME=/usr/local/apache-ant-1.8.4PATH=$ANT_HOME/bin:$PATHexport ANT_HOME 完整的配置如下 123456789JAVA_HOME=/usr/local/jdk7JRE_HOME=/usr/local/jdk7/jreJENKINS_HOME=/var/lib/jenkinsMAVEN_HOME=/usr/local/apache-maven-3.5.0ANT_HOME=/usr/local/apache-ant-1.8.4PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$MAVEN_HOME/bin:$ANT_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport JAVA_HOME JRE_HOME PATH MAVEN_HOME ANT_HOME CLASSPATH JENKINS_HOMEexport LD_LIBRARY_PATH=/usr/apr/lib 注意：ANT的1.9到1.10版本均由jdk1.8版本编译支持，如果用的是jdk1.7，要下载ANT1.8版本。如果是远程进行ant构建需要下载一个jar包，Ant提供的Sshexec和scp任务，由$ANT_HOME/lib/ant-jsch.jar提供支持，但是同样你也要在http://www.jcraft.com/jsch/index.html下载一个依赖包jsch-0.1.24.jar(文件名因版本而不同)，jsch同样也是http://www.sourceforge.net下的一个项目。你需要把下载的jsch-0.1.24拷贝到$ANT_HOME/lib下 Jenkins安装支持war和本地安装两种方式（本文主要说明第二种方式） Jenkins下载地址1https://jenkins.io/index.html 默认情况下，最新版本和长期支持版本可供下载。过去的版本也可以下载。单击下载部分的长期支持版本选项。 Jenkins两种安装方式 Jenkins的war包下载后发布到Tomcat服务器的Webapps下即可配（比较简单） 用于Jenkins的RedHat Linux RPM软件包(第二种方式) 使用Jenkins官网提供的源下载Jenkins，服务器上面执行下面命令： 123[root@localhost jenkins]# sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo[root@localhost jenkins]# sudo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key[root@localhost jenkins]# sudo yum install jenkins 启动Jenkins 1[root@localhost jenkins]# service jenkins start/stop/restart jenkins的默认设置： Jenkins会随系统启动而启动。详情参照/etc/init.d/jenkins Jenkins会创建一个用户叫做jenkins, 如果你修改了user，则要修修改所属者：/var/log/jenkins,/var/lib/jenkins,/var/cache/jenkins 如果遇到问题，查看日志/var/log/jenkins/jenkins.log 配置文件/etc/sysconfig/jenkins 默认启用8080 （本文使用的是8888） 1[root@localhost jenkins]# vi /etc/sysconfig/jenkins 修改如下几个地方：1、JENKINS_USER=”root” 当前用户执行权限。2、JENKINS_JAVA_OPTIONS=”-Dhudson.util.ProcessTree.disable=true” 此处必须要改，防止在启动jenkins 的时候jenkins自动杀死衍生进程，导致服务器始终无法正常启动。3、JENKINS_PORT=”8888” 为了避免和Tomcat端口冲突，修改自己的端口。 安装后直接访问地址：1http://服务器IP地址:8888/]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql自动安装脚本]]></title>
    <url>%2F2019%2F02%2F10%2FMysql%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[执行过程由于频繁安装mysql添加用户配置权限以及编译初始化过程基本一致，所以可以直接编写shell文件来自动执行重复的工作。 MySQL脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#!/bin/bash# 脚本 ：Mysql自动安装配置脚本# 说明 ：安装 mysql-5.6.32-linux-glibc2.5-x86_64.tar.gz on # 环境 ：Centos# 作者 ：Jackie.Yang# 编写日期：2016.09.20# 备份系统文件mv /etc/my.cnf /etc/my.cnf.bakcd /usr/local/cp ./e/my.cnf /etc/echo 解压缩...tar -zxvf mysql-5.6.32-linux-glibc2.5-x86_64.tar.gzsleep 15secho 解压完毕，进程等待...#修改安装名称mv mysql-5.6.32-linux-glibc2.5-x86_64 mysql#进去mysql根目录cd mysqlecho 添加用户groupadd mysqluseradd -g mysql mysqlecho 修改mysql所属用户组chown -R mysql:mysql /usr/local/mysqlecho 开始执行mysql安装。。。# mysql5.7之前版本初始化命令./scripts/mysql_install_db --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql#mysql5.7的初始化命令# ./bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/dataecho 安装等待。。。sleep 15srm my.cnfcp ../m/my.cnf .chown -R mysql:mysql /usr/local/mysql/my.cnfcp support-files/mysql.server /etc/init.d/mysqldchmod +x /etc/init.d/mysqld#echo 添加mysql到系统服务#chkconfig --add mysqld #echo 添加mysql随机启动 #chkconfig mysqld oncd bin#service mysqld start | awk '&#123;print $3&#125;'service mysqld startecho 启动服务中...sleep 5s#判断字符串是否相等service mysqld start | awk '&#123;if("SUCCESS!"== $3) print "Mysql数据库启动成功！" &#125;' ln -s /var/lib/mysql/mysql.sock /tmp/mysql.sockecho 初始化数据库配置...# 5.7数据会生成临时密码：如：XAcLpcVsd9/.；# 修改密码#SET PASSWORD = PASSWORD('123456');./mysql -uroot -e "use mysqlupdate user set password=password('123456') where user='root';GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;flush privileges;quit"echo 配置成功，数据库用户：root； 密码：123456；exit;#service mysqld start | awk '&#123;if("SUCCESS!" != $3) print "Mysql数据库启动失败！" &#125;' 注：mysql5.7之前版本初始化配置表命令./scripts/mysql_install_db，mysql5.7的mysql_install_db命令是在bin目录下，并且建议用mysqld –initialize命令来执行初始化，此脚本针对Mysql5.6及以下版本，5.7等高版本需替换对应版本的初始化脚本。 配置文件 复制修改好的my.cnf到 /etc/下，内容如下： 1234567891011121314151617[mysqld] socket=/tmp/mysql.sock user=mysql basedir = /usr/local/mysql datadir = /usr/local/mysql/data port = 3306 server_id = 121 log_bin = mysql_bin wait_timeout=31536000 interactive_timeout=31536000# Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tengine安装和使用]]></title>
    <url>%2F2019%2F02%2F09%2FTengine%2F</url>
    <content type="text"><![CDATA[Tengine简介Tengine 作为淘宝Web服务器，是基于nginx的进行了订制开发和扩展，也解决了Nginx中upstream无法检查代理后端存活等众多不灵活的问题，本人正是由于此需求了解到了Tengine，并且完成了安装测试调试等一系列问题，最终部署生产应用。 Tengine发展Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。从2011年12月开始，Tengine成为一个开源项目。现在，它由Tengine团队开发和维护。Tengine团队的核心成员来自于淘宝、搜狗等互联网企业。 Tengine功能 继承Nginx-1.6.2的所有特性，兼容Nginx的配置； 动态模块加载（DSO）支持。加入一个模块不再需要重新编译整个Tengine； 支持SO_REUSEPORT选项，建连性能提升为官方nginx的三倍； 支持SPDY v3协议，自动检测同一端口的SPDY请求和HTTP请求； 流式上传到HTTP后端服务器或FastCGI服务器，大量减少机器的I/O压力； 更加强大的负载均衡能力，包括一致性hash模块、会话保持模块，还可以对后端的服务器进行主动健康检查，根据服务器状态自动上线下线，以及动态解析upstream中出现的域名； 输入过滤器机制支持。通过使用这种机制Web应用防火墙的编写更为方便； 支持设置proxy、memcached、fastcgi、scgi、uwsgi在后端失败时的重试次数； 动态脚本语言Lua支持。扩展功能非常高效简单； 支持管道（pipe）和syslog（本地和远端）形式的日志以及日志抽样； 支持按指定关键字(域名，url等)收集Tengine运行状态； 组合多个CSS、JavaScript文件的访问请求变成一个请求； 自动去除空白字符和注释从而减小页面的体积 自动根据CPU数目设置进程个数和绑定CPU亲缘性； 监控系统的负载和资源占用从而对系统进行保护； 显示对运维人员更友好的出错信息，便于定位出错机器； 更强大的防攻击（访问速度限制）模块； 更方便的命令行参数，如列出编译的模块列表、支持的指令等； 可以根据访问文件类型设置过期时间； Tengine下载地址 下载地址：http://tengine.taobao.org/download.html 也可以通过wget直接下载到指定服务器： wget http://tengine.taobao.org/download/tengine-2.1.0.tar.gz 本人使用的是tengine版本2.1.0，相对来说比较稳定。 Tengine环境依赖由于Tengine安装需要使用源代码自行编译，所以在安装前需要安装必要的编译工具： 1、PCRE PCRE(Perl Compatible Regular Expressions)是一个Perl库，包括 perl 兼容的正则表达式库。nginx rewrite依赖于PCRE库，所以在安装Tengine前一定要先安装PCRE。 123456cd /usr/local/ wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.42.tar.gz tar -zxvf pcre-8.42.tar.gz cd pcre-8.42 ./configure --prefix=/usr/local/pcre make &amp;&amp; make install 2、Zlib Zlib是提供资料压缩之用的函式库，当Tengine想启用GZIP压缩的时候就需要使用到Zlib 123456cd /usr/local/ wget http://zlib.net/zlib-1.2.11.tar.gz tar zxvf zlib-1.2.11.tar.gz cd zlib-1.2.11 ./configure --prefix=/usr/local/zlib make &amp;&amp; make install 3、jemalloc jemalloc是一个很好的内存管理工具，使用jemalloc可以更好的优化Tengine的内存管理。 123456cd /usr/local/ wget https://github.com/jemalloc/jemalloc/releases/download/5.0.1/jemalloc-5.0.1.tar.gz tar -zxvf jemalloc-5.0.1.tar.gz cd jemalloc-5.0.1 ./configure --prefix=/usr/local/jemalloc make &amp;&amp; make install 4、OpenSSL OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。安装OpenSSL主要是为了让tengine支持Https的访问请求。 123456cd /usr/local/ wget https://www.openssl.org/source/openssl-fips-2.0.16.tar.gz tar -zxvf openssl-fips-2.0.16 cd openssl-fips-2.0.16./config --prefix=/usr/local/openssl make &amp;&amp; make install 5、gcc yum install gcc gcc-c++ 注：源码的安装一般由3个步骤组成：配置(configure)、编译(make)、安装(make install)。Configure是一个可执行脚本，它有很多选项，在待安装的源码路径下使用命令./configure –help输出详细的选项列表。其中–prefix选项是配置安装的路径，如果不配置该选项，安装后可执行文件默认放在/usr /local/bin，库文件默认放在/usr/local/lib，配置文件默认放在/usr/local/etc，其它的资源文件放在/usr /local/share，比较凌乱。如果配置–prefix，如：./configure –prefix=/usr/local/test，可以把所有资源文件放在/usr/local/test的路径中，不会杂乱。用了—prefix选项的另一个好处是卸载软件或移植软件。当某个安装的软件不再需要时，只须简单的删除该安装目录，就可以把软件卸载得干干净净；移植软件只需拷贝整个目录到另外一个机器即可（相同的操作系统）。当然要卸载程序，也可以在原来的make目录下用一次make uninstall，但前提是make文件指定过uninstall。 以上依赖环境安装后的目录格式如下： 最后进入tengine目录执行： 12345678./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-pcre=/usr/local/pcre-8.42 --with-openssl=/usr/local/openssl --with-zlib=/usr/local/zlib --with-jemalloc=/usr/local/jemalloc make &amp;&amp; make install 由于以上安装过程过于麻烦，笔者使用了简化安装，yum安装依赖环境后，直接执行./configure 123456yum -y install gcc automake autoconf libtool make yum -y install pcre-develyum -y install openssl openssl-devel cd /usr/local/tengine-2.2.3./configuremake &amp;&amp; make install 启动tengine 第一种启动方式会默认加载nginx下的配置文件 ./nginx/sbin/nginx -s reload 第二种启动方式指定tengine的配置文件/usr/local/nginx/sbin/nginx -c /usr/local/tengine-2.2.3/conf/nginx.conf]]></content>
      <categories>
        <category>开源软件</category>
      </categories>
      <tags>
        <tag>Tengine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyCat 配置详解]]></title>
    <url>%2F2018%2F12%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[MyCat是一款基于阿里开源产品Cobar而研发的开源数据库分库分表中间件（基于Java语言开发）。MyCat是一个开源的分布式数据库系统，是一个实现了MySQL协议的服务器，前端用户可以把它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生协议与多个MySQL服务器通信，也可以用JDBC协议与大多数主流数据库服务器通信，其核心功能是分表分库，即将一个大表水平分割为N个小表，存储在后端MySQL服务器里或者其他数据库里。也可以这样理解：数据库是对底层存储文件的抽象，而Mycat是对数据库的抽象。 MyCat关键特性 支持SQL92标准 支持MySQL、Oracle、DB2、SQL Server、PostgreSQL等DB的常见SQL语法 遵守Mysql原生协议，跨语言，跨平台，跨数据库的通用中间件代理 基于心跳的自动故障切换，支持读写分离，支持MySQL主从，以及galera cluster集群 支持Galera for MySQL集群，Percona Cluster或者MariaDB cluster 基于Nio实现，有效管理线程，解决高并发问题 支持数据的多片自动路由与聚合，支持sum,count,max等常用的聚合函数,支持跨库分页 支持单库内部任意join，支持跨库2表join，甚至基于caltlet的多表join 支持通过全局表，ER关系的分片策略，实现了高效的多表join查询 支持多租户方案 支持分布式事务（弱xa） 支持XA分布式事务（1.6.5） 支持全局序列号，解决分布式下的主键生成问题 分片规则丰富，插件化开发，易于扩展 强大的web，命令行监控 支持前端作为MySQL通用代理，后端JDBC方式支持MySQL、PostgreSQL、Oracle、DB2、SQLServer、MongoDB、巨杉 支持密码加密 支持服务降级 支持IP白名单 支持SQL黑名单、sql注入攻击拦截 支持prepare预编译指令（1.6） 支持非堆内存(Direct Memory)聚合计算（1.6） 支持PostgreSQL的native协议（1.6） 支持mysql和oracle存储过程，out参数、多结果集返回（1.6） 支持zookeeper协调主从切换、zk序列、配置zk化（1.6） 支持库内分表（1.6） 集群基于ZooKeeper管理，在线升级，扩容，智能优化，大数据处理（2.0开发版） MyCat的目标低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。 MyCat的优势基于阿里开源的Cobar产品而研发，Cobar的稳定性、可靠性、优秀的架构和性能，以及众多成熟的使用案例使得MyCAT一开始就拥有一个很好的起点，站在巨人的肩膀上，能看到更远。 广泛吸取业界优秀的开源项目和创新思路，将其融入到MyCAT的基因中，使得MyCAT在很多方面都领先于目前其他一些同类的开源项目，甚至超越某些商业产品。 MyCAT背后有一只强大的技术团队，其参与者都是5年以上资深软件工程师、架构师、DBA等，优秀的技术团队保证了MyCAT的产品质量。 MyCAT并不依托于任何一个商业公司，因此不像某些开源项目，将一些重要的特性封闭在其商业产品中，使得开源项目成了一个摆设。 MyCat的架构 MyCAT使用MySQL的通讯协议模拟成一个MySQL服务器，并建立了完整的Schema（数据库）、Table （数据表）、User（用户）的逻辑模型，并将这套逻辑模型映射到后端的存储节点DataNode（MySQL Instance）上的真实物理库中，这样一来，所有能使用MySQL的客户端以及编程语言都能将MyCAT当成是MySQLServer来使用，不必开发新的客户端协议。 当MyCAT收到一个客户端发送的SQL请求时，会先对SQL进行语法分析和检查，分析的结果用于SQL路由，SQL路由策略支持传统的基于表格的分片字段方式进行分片，也支持独有的基于数据库E-R关系的分片策略，对于路由到多个数据节点（DataNode）的SQL，则会对收到的数据集进行“归并”然后输出到客户端。SQL执行的过程，简单的说，就是把SQL通过网络协议发送给后端的真正的数据库上进行执行，对于MySQL Server来说，是通过MySQL网络协议发送报文，并解析返回的结果，若SQL不涉及到多个分片节点，则直接返回结果，写入客户端的SOCKET流中，这个过程是非阻塞模式（NIO）。 DataNode是MyCAT的逻辑数据节点，映射到后端的某一个物理数据库的一个Database，为了做到系统高可用，每个DataNode可以配置多个引用地址（DataSource），当主DataSource被检测为不可用时，系统会自动切换到下一个可用的DataSource上，这里的DataSource即可认为是Mysql的主从服务器的地址。 MyCat的初始化 MyCat的逻辑库与任何一个传统的关系型数据库一样，MyCAT也提供了“数据库”的定义，并有用户授权的功能，下面是MyCAT逻辑库相关的一些概念： schema:逻辑库，与MySQL中的Database（数据库）对应，一个逻辑库中定义了所包括的Table。 table：表，即物理数据库中存储的某一张表，与传统数据库不同，这里的表格需要声明其所存储的逻辑数据节点DataNode，这是通过表格的分片规则定义来实现的，table可以定义其所属的“子表(childTable)”，子表的分片依赖于与“父表”的具体分片地址，简单的说，就是属于父表里某一条记录A的子表的所有记录都与A存储在同一个分片上。 分片规则：是一个字段与函数的捆绑定义，根据这个字段的取值来返回所在存储的分片（DataNode）的序号，每个表格可以定义一个分片规则，分片规则可以灵活扩展，默认提供了基于数字的分片规则，字符串的分片规则等。 dataNode: MyCAT的逻辑数据节点，是存放table的具体物理节点，也称之为分片节点，通过DataSource来关联到后端某个具体数据库上，一般来说，为了高可用性，每个DataNode都设置两个DataSource，一主一从，当主节点宕机，系统自动切换到从节点。 dataHost：定义某个物理库的访问地址，用于捆绑到dataNode上。 MyCAT目前通过配置文件的方式来定义逻辑库和相关配置： MYCAT_HOME/conf/schema.xml中定义逻辑库，表、分片节点等内容； MYCAT_HOME/conf/rule.xml中定义分片规则； MYCAT_HOME/conf/server.xml中定义用户以及系统相关变量，如端口等。 MyCat的核心配置解析 schema.xml相关参数说明： &lt;?xml version="1.0"?&gt; &lt;!DOCTYPE mycat:schema SYSTEM "schema.dtd"&gt; &lt;mycat:schema xmlns:mycat="http://io.mycat/"&gt; &lt;!-- 定义一个MyCat的模式，逻辑数据库名称hsg_oa --&gt; &lt;!-- “checkSQLschema”：描述的是当前的连接是否需要检测数据库的模式 --&gt; &lt;!-- “sqlMaxLimit”：表示返回的最大的数据量的行数 --&gt; &lt;!-- “dataNode="dn1"”：该操作使用的数据节点是dn1的逻辑名称 --&gt; &lt;schema name="hsg_oa" checkSQLschema="false" sqlMaxLimit="100" dataNode="dn1"/&gt; &lt;!-- 定义数据的操作节点 --&gt; &lt;!-- “dataHost="localhost1"”：定义数据节点的逻辑名称 --&gt; &lt;!-- “database="mldn"”：定义数据节点要使用的数据库名称 --&gt; &lt;dataNode name="dn1" dataHost="localhost1" database="hsg_oa" /&gt; &lt;!-- 定义数据节点，包括了各种逻辑项的配置 --&gt; &lt;dataHost name="localhost1" maxCon="1000" minCon="10" balance="0" writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100"&gt; &lt;!-- 配置真实MySQL与MyCat的心跳 --&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- 配置真实的MySQL的连接路径 --&gt; &lt;writeHost host="hostM1" url="192.168.1.128:3306" user="root" password="123456"&gt;&lt;/writeHost&gt; &lt;/dataHost&gt; &lt;/mycat:schema&gt; dataHost标签属性说明 balance:负载均衡类型0：不开启读写分离机制，所有读操作都发送到当前可用的writeHost上 1：全部的readHost与stand by writeHost参与select语句的负载均衡， 2：所有读操作都随机在writeHost、readHost上分发 3：所有读请求随机分发到writeHost对应的readHost执行，writeHost不负担读压力 writeType:负载均衡类型 0：所有写操作发送到配置的第一个writeHost，当第一个writeHost宕机时，切换到第二个writeHost，重新启动后以切换后的为准，切换记录在配置文件：dnindex.properties中 1：所有写操作都随发送到配置的writeHost 2：尚未实现 switchType:切换方式 -1：不自动切换1：自动切换（默认） 2：基于MySql主从同步的状态来决定是否切换 server.xml相关参数说明： &lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;&lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;system&gt; &lt;!-- 1为开启实时统计、0为关闭 如果使用Mycat-eye监控SQL则需要开启此项--&gt; &lt;code&gt;&lt;property name=&quot;useSqlStat&quot;&gt;0&lt;/property&gt;&lt;/code&gt; &lt;!-- 1为开启全加班一致性检测、0为关闭 --&gt; &lt;property name=&quot;useGlobleTableCheck&quot;&gt;0&lt;/property&gt; &lt;property name=&quot;sequnceHandlerType&quot;&gt;2&lt;/property&gt; &lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&gt; &lt;property name=&quot;handleDistributedTransactions&quot;&gt;0&lt;/property&gt; &lt;!--off heap for merge/order/group/limit 1开启 0关闭 --&gt; &lt;property name=&quot;useOffHeapForMerge&quot;&gt;1&lt;/property&gt; &lt;!--单位为m--&gt; &lt;property name=&quot;memoryPageSize&quot;&gt;1m&lt;/property&gt; &lt;!--溢出文件缓冲区大小 单位为k--&gt; &lt;property name=&quot;spillsFileBufferSize&quot;&gt;1k&lt;/property&gt; &lt;property name=&quot;useStreamOutput&quot;&gt;0&lt;/property&gt; &lt;!--系统预留内存大小 单位为m--&gt; &lt;property name=&quot;systemReserveMemorySize&quot;&gt;384m&lt;/property&gt; &lt;!--是否采用zookeeper协调切换 --&gt; &lt;property name=&quot;useZKSwitch&quot;&gt;true&lt;/property&gt; &lt;!-- root 登录设置 --&gt; &lt;user name=&quot;root&quot;&gt; &lt;property name=&quot;password&quot;&gt;Hsg@123456&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;hsg_wx&lt;/property&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;false&quot;&gt; &lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt; &lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt; &lt;/user&gt; &lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt;user&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;hsg_wx&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt; &lt;/user&gt; MyCat常用命令 ./mycat restart 重启服务 ./mycat pause 暂停 ./mycat status 查看启动状态 ./mycat start 启动 ./mycat stop 停止 mysql -uroot -p（账号密码） -h（所在机器IP） -P8066 -D（逻辑库名） 终端登录]]></content>
      <categories>
        <category>MyCat</category>
      </categories>
      <tags>
        <tag>MyCat</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存结构解析]]></title>
    <url>%2F2018%2F12%2F24%2FJVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[内存结构图 Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。 Java堆（Heap） 对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区（Method Area） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 程序计数器（Program Counter Register） 程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 JVM栈（JVM Stacks） 与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈（Native Method Stacks） 本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署到Github服务器]]></title>
    <url>%2F2018%2F12%2F24%2Fnew-Day%2F</url>
    <content type="text"><![CDATA[Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。正常来说，不需要部署到我们的服务器上，我们的服务器上保存的，其实是基于在hexo通过markdown编写的文章，然后hexo帮我们生成静态的html页面，然后，将生成的html上传到我们的服务器。为了更加方便的实现本地到git服务器的部署同步问题，因此可以通过如下操作配置实现本地编写文章后，直接部署到Github服务器，通过Github域名来访问,达到快捷、高效、一劳永逸的效果。 配置文件修改_config.yml文件中对应的repo值如下 1234deploy:type: gitrepository: git@github.com:yangzhiwen911/yangzhiwen911.github.io.gitbranch: master 注：repository的值 即为github项目中的ssh地址，直接复制过来即可。 生成SSH git bash中配置name(git的用户名)和email（git的邮箱） 12git config --global user.name "yourname"git config --global user.email "youremail" 进入~目录，执行ssh-keygen，生成.ssh文件夹会有一对公钥和私钥,复制公钥内容到Github 1ssh-keygen -t rsa -C "youremail" #生成ssh对应key，直接输入回车并设置密码即可。 Github对应的ssh and key地址入口： 在gitbash中，查看是否成功 1ssh -T git@github.com 当出现你的Github用户名的时候，即为成功，可以进行下一步操作。 安装插件 在生成以及部署文章之前，需要安装一个插件，这样才能用命令部署到GitHub。 1npm install hexo-deployer-git --save 最后执行部署： 12hexo dhexo s 至此，部署过程结束，可以直接通过http://yourname.github.io 网址查看你的个人博客了！ 部署问题 如果出现缺少模块等报错问题，直接使用hexo install 下载命令即可。 如果出现模板渲染等报错问题，需检查md文件是否有标签未闭合等语法问题。 命令大全 Hexo常用命令： 12345678910hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #将.deploy目录部署到GitHubhexo version #查看hexo版本号hexo --debug #调试模式，输出所有日志信息hexo --safe #安全模式，禁用所有的插件和脚本hexo --silent #无日志输出模式hexo --config config-path #指定配置文件，代替默认的_config.yml Hexo缩略语： 12345hexo n == hexo new #创建一个新的blog页面hexo g == hexo generate #生成静态文件hexo s == hexo server #启动本地服务器hexo d == hexo deploy #部署网站项目hexo v == hexo version #hexo版本 Hexo复合命令： 1234hexo deploy -ghexo server -ghexo server -d (hexo s ) #启动本地服务器hexo generate -w #watch参数的缩写，source/_post中的md文件有改动，就会自动publish Hexo清除命令： 1hexo clean 注：Hexo原理就是hexo在执行hexo generate时会在本地先把博客生成的一套静态站点放到public文件夹中，在执行hexo deploy时将其复制到.deploy文件夹中。Github的版本库通常建议同时附上README.md说明文件，但是hexo默认情况下会把所有md文件解析成html文件，所以即使你在线生成了README.md，它也会在你下一次部署时被删去。怎么解决呢？在执行hexo deploy前把在本地写好的README.md文件复制到.deploy文件夹中，再去执行hexo deploy。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>MyCat</tag>
        <tag>Github</tag>
        <tag>ssh</tag>
        <tag>rsa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java技术栈梳理分享]]></title>
    <url>%2F2018%2F12%2F24%2FJackie-Yang%2F</url>
    <content type="text"><![CDATA[Java面试分析 结合底层实现原理梳理了一些面试点：]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
